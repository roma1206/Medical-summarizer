{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b37bd567277445d08875c5e608da5105":{"model_module":"@jupyter-widgets/controls","model_name":"TextareaModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextareaModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextareaView","continuous_update":true,"description":"Input Text:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_16a080bf272f44f9b414a7ba7a574420","placeholder":"Paste or type your medical transcript here...","rows":null,"style":"IPY_MODEL_fc3e5160c9724bba9be0f9c45b0ba8b8","value":"HISTORY OF PRESENT ILLNESS: , The patient is a 62-year old male with a Gleason score 8 adenocarcinoma of the prostate involving the left and right lobes.  He has a PSA of 3.1, with a prostate gland size of 41 grams.  This was initially found on rectal examination with a nodule on the right side of the prostate, showing enlargement relative to the left.  He has undergone evaluation with a bone scan that showed a right parietal lesion uptake and was seen by Dr. XXX and ultimately underwent an open biopsy that was not malignant.  Prior to this, he has also had a ProstaScint scan that was negative for any metastatic disease.  Again, he is being admitted to undergo a radical prostatectomy, the risks, benefits, and alternatives of which have been discussed, including that of bleeding, and a blood transfusion.,PAST MEDICAL HISTORY: , Coronary stenting.  History of high blood pressure, as well.  He has erectile dysfunction and has been treated with Viagra.,MEDICATIONS: , Lisinopril, Aspirin, Zocor, and Prilosec.,ALLERGIES:,  Penicillin.,SOCIAL HISTORY:,  He is not a smoker.  He does drink six beers a day.,REVIEW OF SYSTEMS: , Remarkable for his high blood pressure and drug allergies, but otherwise unremarkable, except for some obstructive urinary symptoms, with an AUA score of 19.,PHYSICAL EXAMINATION:,HEENT:  Examination unremarkable.,Breasts:  Examination deferred.,Chest:  Clear to auscultation.,Cardiac:  Regular rate and rhythm.,Abdomen:  Soft and nontender.  He has no hernias.,Genitourinary:  There is a normal-appearing phallus, prominence of the right side of prostate.,Extremities:  Examination unremarkable.,Neurologic:  Examination nonfocal.,IMPRESSION:,1.  Adenocarcinoma of the prostate.,2.  Erectile dysfunction.,PLAN:  ,The patient will undergo a bilateral pelvic lymphadenectomy and radical retropubic prostatectomy.  The risks, benefits, and alternatives of this have been discussed.  He understands and asks that I proceed ahead.  We also discussed bleeding and blood transfusions, and the risks, benefits and alternatives thereof."}},"16a080bf272f44f9b414a7ba7a574420":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"200px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"600px"}},"fc3e5160c9724bba9be0f9c45b0ba8b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4459c39309e4982a403e1644eae0c24":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Generate Summary","disabled":false,"icon":"","layout":"IPY_MODEL_2569829f1ea64f2c84785aee344b3de6","style":"IPY_MODEL_d8b4d24f9ff34d37832cb468c7a3d8c6","tooltip":""}},"2569829f1ea64f2c84785aee344b3de6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8b4d24f9ff34d37832cb468c7a3d8c6":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"5d9ef5ccf93b44b089784e4ed009c3a4":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d3d60cb5322405692c95cbd4474c568","placeholder":"​","style":"IPY_MODEL_007237abfaa2453a936f375e0f61cac7","value":"Generated Summary: "}},"4d3d60cb5322405692c95cbd4474c568":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"007237abfaa2453a936f375e0f61cac7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b7e998232374f0f98531df830acae62":{"model_module":"@jupyter-widgets/controls","model_name":"TextareaModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextareaModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextareaView","continuous_update":true,"description":"","description_tooltip":null,"disabled":true,"layout":"IPY_MODEL_7fd6bb11af0a4b3a906bee3498117963","placeholder":"​","rows":null,"style":"IPY_MODEL_0a8a051c84b7485dbd496a3961b463a9","value":". The patient is a 52 - year - old male with a history of chronic otitis media , who was found to have history of papillomatosis onset a large bilateral inferior 4 cm area which has had an unknown and was found to have history of consciousness . He had an unknown at this , which was found to have history of chest pain and also for a large . He had an unknown at this was thought of consciousness . He had an unknown at the last six weeks . He had been having his parents . He had been noted to have large . He had been noted to have history of his parents . He had a large . He had a large ."}},"7fd6bb11af0a4b3a906bee3498117963":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"100px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"600px"}},"0a8a051c84b7485dbd496a3961b463a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Download dataset and save in the drive and then mount the drive to colab for better workflow than uploading file in every runtime.\n"],"metadata":{"id":"EkyUdR9gTcZS"}},{"cell_type":"code","source":["# Mount Google Drive to save/load model\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_4z21whSoZy","outputId":"fb800425-bb4c-4116-afd0-96a6846f78f7","executionInfo":{"status":"ok","timestamp":1763704673737,"user_tz":-330,"elapsed":1865,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Install required libraries\n","!pip install torch tokenizers accelerate -U\n","!pip install rouge_score\n","!pip install torch tokenizers\n","\n","# Install ngrok for tunneling\n","!pip install pyngrok\n","\n","# Install required packages\n","!pip install streamlit tokenizers rouge-score\n","\n","# Install localtunnel\n","!npm install -g localtunnel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NspCVxv0S0V_","outputId":"023c7328-dcc8-4e37-8cf8-4d18ca256ef4","executionInfo":{"status":"ok","timestamp":1763705650749,"user_tz":-330,"elapsed":25491,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n","Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n","Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.5)\n","Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n","Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n","Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.5)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n","Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n","Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.11.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n","added 22 packages in 2s\n","\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n","\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n","\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n","\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"]}]},{"cell_type":"markdown","source":["# Small Model with less accuracy (30 Epochs)"],"metadata":{"id":"JJh0sNsES2G2"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfOxAz44RaoC","outputId":"ac9afcc8-f054-42a4-bcaf-93fc57b8746f"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-4193130313.py:124: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-4193130313.py:156: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","/tmp/ipython-input-4193130313.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 6.7389, Val Loss: 6.0394, ROUGE-1: 0.1594\n","Epoch 2, Train Loss: 5.4838, Val Loss: 5.3924, ROUGE-1: 0.1895\n","Epoch 3, Train Loss: 4.7665, Val Loss: 4.9051, ROUGE-1: 0.1689\n","Epoch 4, Train Loss: 4.1942, Val Loss: 4.5177, ROUGE-1: 0.2152\n","Epoch 5, Train Loss: 3.6936, Val Loss: 4.1553, ROUGE-1: 0.1724\n","Epoch 6, Train Loss: 3.2391, Val Loss: 3.8263, ROUGE-1: 0.2582\n","Epoch 7, Train Loss: 2.8137, Val Loss: 3.4976, ROUGE-1: 0.2463\n","Epoch 8, Train Loss: 2.4272, Val Loss: 3.2177, ROUGE-1: 0.2285\n","Epoch 9, Train Loss: 2.0613, Val Loss: 2.9594, ROUGE-1: 0.2733\n","Epoch 10, Train Loss: 1.7543, Val Loss: 2.6839, ROUGE-1: 0.2976\n","Epoch 11, Train Loss: 1.4668, Val Loss: 2.4353, ROUGE-1: 0.3854\n","Epoch 12, Train Loss: 1.2387, Val Loss: 2.2166, ROUGE-1: 0.3432\n","Epoch 13, Train Loss: 1.0515, Val Loss: 2.0342, ROUGE-1: 0.5252\n","Epoch 14, Train Loss: 0.8718, Val Loss: 1.8536, ROUGE-1: 0.3287\n","Epoch 15, Train Loss: 0.7507, Val Loss: 1.7251, ROUGE-1: 0.3973\n","Epoch 16, Train Loss: 0.6383, Val Loss: 1.5869, ROUGE-1: 0.4039\n","Epoch 17, Train Loss: 0.5433, Val Loss: 1.4766, ROUGE-1: 0.6199\n","Epoch 18, Train Loss: 0.4823, Val Loss: 1.4544, ROUGE-1: 0.4267\n","Epoch 19, Train Loss: 0.4282, Val Loss: 1.4027, ROUGE-1: 0.4201\n","Epoch 20, Train Loss: 0.3783, Val Loss: 1.3596, ROUGE-1: 0.4166\n","Epoch 21, Train Loss: 0.3411, Val Loss: 1.3402, ROUGE-1: 0.4239\n","Epoch 22, Train Loss: 0.3111, Val Loss: 1.3030, ROUGE-1: 0.4254\n","Epoch 23, Train Loss: 0.2863, Val Loss: 1.3052, ROUGE-1: 0.4413\n","Epoch 24, Train Loss: 0.2592, Val Loss: 1.2782, ROUGE-1: 0.2943\n","Epoch 25, Train Loss: 0.2396, Val Loss: 1.3008, ROUGE-1: 0.4824\n","Epoch 26, Train Loss: 0.2366, Val Loss: 1.2940, ROUGE-1: 0.4133\n","Epoch 27, Train Loss: 0.2224, Val Loss: 1.2821, ROUGE-1: 0.3582\n","Epoch 28, Train Loss: 0.2011, Val Loss: 1.3141, ROUGE-1: 0.4304\n","Epoch 29, Train Loss: 0.1941, Val Loss: 1.3075, ROUGE-1: 0.3550\n","Epoch 30, Train Loss: 0.1799, Val Loss: 1.3103, ROUGE-1: 0.3471\n","Model saved to: /content/drive/MyDrive/Transformer\n"]}],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import math\n","import os\n","from rouge_score import rouge_scorer\n","\n","# Step 1: Load and prepare the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Transformer/mtsamples.csv')\n","df = df[['transcription', 'description']].dropna()\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","\n","# Step 2: Train a WordPiece tokenizer on the dataset\n","texts = list(df['text']) + list(df['summary'])\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","tokenizer.train_from_iterator(texts, trainer=trainer)\n","\n","# Save tokenizer\n","tokenizer_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(tokenizer_path, exist_ok=True)\n","tokenizer.save(os.path.join(tokenizer_path, \"tokenizer.json\"))\n","\n","# Encode dataset\n","def encode_texts(texts, summaries, max_input_length=512, max_target_length=128):\n","    input_encodings = []\n","    target_encodings = []\n","    for text, summary in zip(texts, summaries):\n","        input_ids = tokenizer.encode(text).ids\n","        target_ids = tokenizer.encode(summary).ids\n","        if len(input_ids) > max_input_length:\n","            input_ids = input_ids[:max_input_length]\n","        if len(target_ids) > max_target_length:\n","            target_ids = target_ids[:max_target_length]\n","        input_encodings.append(input_ids)\n","        target_encodings.append(target_ids)\n","    return input_encodings, target_encodings\n","\n","input_encodings, target_encodings = encode_texts(df['text'], df['summary'])\n","\n","# Pad sequences\n","def pad_sequences(sequences, max_length, pad_token_id):\n","    padded = []\n","    for seq in sequences:\n","        if len(seq) < max_length:\n","            seq = seq + [pad_token_id] * (max_length - len(seq))\n","        padded.append(seq[:max_length])\n","    return padded\n","\n","max_input_length = 512\n","max_target_length = 128\n","pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n","input_encodings = pad_sequences(input_encodings, max_input_length, pad_token_id)\n","target_encodings = pad_sequences(target_encodings, max_target_length, pad_token_id)\n","\n","# Convert to tensors\n","input_tensors = torch.tensor(input_encodings, dtype=torch.long)\n","target_tensors = torch.tensor(target_encodings, dtype=torch.long)\n","\n","# Split into train and validation (80-20)\n","train_size = int(0.8 * len(input_tensors))\n","train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n","train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n","\n","# Step 3: Define Transformer model from scratch\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024, dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","        src = self.embedding(src) * math.sqrt(self.d_model)\n","        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        tgt = self.pos_encoder(tgt)\n","        output = self.transformer(src, tgt, src_mask, tgt_mask)\n","        output = self.fc_out(output)\n","        return output\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","        return mask\n","\n","# Instantiate model\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(vocab_size=vocab_size).cuda()\n","\n","# Step 4: Training setup\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n","scaler = GradScaler()\n","\n","# Create dataset and dataloader\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs = inputs\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': self.inputs[idx], 'labels': self.targets[idx]}\n","\n","train_dataset = MedicalDataset(train_inputs, train_targets)\n","val_dataset = MedicalDataset(val_inputs, val_targets)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)\n","\n","# Step 5: Training loop\n","def train_epoch(model, loader, optimizer, criterion, scaler):\n","    model.train()\n","    total_loss = 0\n","    for batch in loader:\n","        optimizer.zero_grad()\n","        src = batch['input_ids'].cuda()\n","        tgt = batch['labels'].cuda()\n","        tgt_input = tgt[:, :-1]\n","        tgt_output = tgt[:, 1:]\n","        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","        with autocast():\n","            output = model(src, tgt_input, tgt_mask=tgt_mask)\n","            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in loader:\n","            src = batch['input_ids'].cuda()\n","            tgt = batch['labels'].cuda()\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","            with autocast():\n","                output = model(src, tgt_input, tgt_mask=tgt_mask)\n","                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","            total_loss += loss.item()\n","\n","            # Decode for ROUGE\n","            preds = torch.argmax(output, dim=-1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(tgt_output.cpu().numpy())\n","    return total_loss / len(loader), all_preds, all_labels\n","\n","# Step 6: Train the model\n","num_epochs = 50  # More epochs due to training from scratch\n","for epoch in range(num_epochs):\n","    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler)\n","    val_loss, val_preds, val_labels = evaluate(model, val_loader, criterion)\n","\n","    # Compute ROUGE scores\n","    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in val_preds]\n","    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in val_labels]\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    rouge_scores = [scorer.score(label, pred) for pred, label in zip(decoded_preds, decoded_labels)]\n","    rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n","\n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, ROUGE-1: {rouge1:.4f}\")\n","\n","# Step 7: Save the model\n","model_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(model_path, exist_ok=True)\n","torch.save(model.state_dict(), os.path.join(model_path, \"model.pt\"))\n","print(f\"Model saved to: {model_path}\")"]},{"cell_type":"code","source":["import torch\n","from tokenizers import Tokenizer\n","import os\n","from IPython.display import display, HTML\n","from ipywidgets import widgets\n","\n","# Load tokenizer\n","tokenizer_path = \"/content/drive/MyDrive/Transformer/tokenizer.json\"\n","tokenizer = Tokenizer.from_file(tokenizer_path)\n","\n","# Define Transformer model (same as training)\n","class PositionalEncoding(torch.nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(torch.nn.Module):\n","    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024, dropout=0.1):\n","        super().__init__()\n","        self.embedding = torch.nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        self.transformer = torch.nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.fc_out = torch.nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","        src = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n","        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n","        src = self.pos_encoder(src)\n","        tgt = self.pos_encoder(tgt)\n","        output = self.transformer(src, tgt, src_mask, tgt_mask)\n","        output = self.fc_out(output)\n","        return output\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","        return mask\n","\n","# Load model\n","model_path = \"/content/drive/MyDrive/Transformer/model.pt\"\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(vocab_size=vocab_size).cuda()\n","model.load_state_dict(torch.load(model_path))\n","model.eval()\n","\n","# Function to generate summary\n","def generate_summary(text, max_length=128):\n","    # Encode input\n","    input_ids = tokenizer.encode(text).ids\n","    input_ids = input_ids[:512]  # Truncate to max input length\n","    input_ids = input_ids + [tokenizer.token_to_id(\"[PAD]\")] * (512 - len(input_ids))\n","    input_tensor = torch.tensor([input_ids], dtype=torch.long).cuda()\n","\n","    # Generate output\n","    generated_ids = [tokenizer.token_to_id(\"[CLS]\")]\n","    for _ in range(max_length):\n","        tgt_tensor = torch.tensor([generated_ids], dtype=torch.long).cuda()\n","        tgt_mask = model.generate_square_subsequent_mask(tgt_tensor.size(1)).cuda()\n","        with torch.no_grad():\n","            output = model(input_tensor, tgt_tensor, tgt_mask=tgt_mask)\n","        next_token = torch.argmax(output[:, -1, :], dim=-1).item()\n","        generated_ids.append(next_token)\n","        if next_token == tokenizer.token_to_id(\"[SEP]\"):\n","            break\n","\n","    # Decode output\n","    summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n","    return summary\n","\n","# Option 1: Colab Form Input (recommended for presentation)\n","text_input = widgets.Textarea(\n","    value='',\n","    placeholder='Paste or type your medical transcript here...',\n","    description='Input Text:',\n","    layout={'width': '600px', 'height': '200px'}\n",")\n","output_label = widgets.Label(value=\"Generated Summary: \")\n","output_text = widgets.Textarea(\n","    value='',\n","    disabled=True,\n","    layout={'width': '600px', 'height': '100px'}\n",")\n","button = widgets.Button(description=\"Generate Summary\")\n","\n","def on_button_clicked(b):\n","    if text_input.value.strip():\n","        summary = generate_summary(text_input.value)\n","        output_text.value = summary\n","    else:\n","        output_text.value = \"Please enter some text to summarize.\"\n","\n","button.on_click(on_button_clicked)\n","\n","# Display form\n","display(text_input, button, output_label, output_text)"],"metadata":{"id":"mtYOnP0ARsSy","colab":{"base_uri":"https://localhost:8080/","height":387,"referenced_widgets":["b37bd567277445d08875c5e608da5105","16a080bf272f44f9b414a7ba7a574420","fc3e5160c9724bba9be0f9c45b0ba8b8","d4459c39309e4982a403e1644eae0c24","2569829f1ea64f2c84785aee344b3de6","d8b4d24f9ff34d37832cb468c7a3d8c6","5d9ef5ccf93b44b089784e4ed009c3a4","4d3d60cb5322405692c95cbd4474c568","007237abfaa2453a936f375e0f61cac7","1b7e998232374f0f98531df830acae62","7fd6bb11af0a4b3a906bee3498117963","0a8a051c84b7485dbd496a3961b463a9"]},"outputId":"a06bfbfb-6a59-4f3c-e18f-27eb4483af84"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Textarea(value='', description='Input Text:', layout=Layout(height='200px', width='600px'), placeholder='Paste…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b37bd567277445d08875c5e608da5105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Generate Summary', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4459c39309e4982a403e1644eae0c24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Label(value='Generated Summary: ')"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d9ef5ccf93b44b089784e4ed009c3a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Textarea(value='', disabled=True, layout=Layout(height='100px', width='600px'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b7e998232374f0f98531df830acae62"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Large Model with more accuracy (100 Epochs)"],"metadata":{"id":"Qf0gKwbaTOkg"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import math\n","import os\n","from rouge_score import rouge_scorer\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","# Step 1: Load and prepare the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Transformer/mtsamples.csv')\n","df = df[['transcription', 'description']].dropna()\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","\n","# Append [SEP] to summaries for consistent training\n","df['summary'] = df['summary'].apply(lambda x: x + ' [SEP]')\n","\n","# Step 2: Train a WordPiece tokenizer on the dataset\n","texts = list(df['text']) + list(df['summary'])\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","tokenizer.train_from_iterator(texts, trainer=trainer)\n","\n","# Save tokenizer\n","tokenizer_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(tokenizer_path, exist_ok=True)\n","tokenizer.save(os.path.join(tokenizer_path, \"tokenizer.json\"))\n","\n","# Encode dataset\n","def encode_texts(texts, summaries, max_input_length=512, max_target_length=256):\n","    input_encodings = []\n","    target_encodings = []\n","    for text, summary in zip(texts, summaries):\n","        input_ids = tokenizer.encode(text).ids\n","        target_ids = tokenizer.encode(summary).ids\n","        input_ids = input_ids[:max_input_length]\n","        target_ids = target_ids[:max_target_length]\n","        input_encodings.append(input_ids)\n","        target_encodings.append(target_ids)\n","    return input_encodings, target_encodings\n","\n","input_encodings, target_encodings = encode_texts(df['text'], df['summary'])\n","\n","# Pad sequences\n","def pad_sequences(sequences, max_length, pad_token_id):\n","    padded = []\n","    for seq in sequences:\n","        if len(seq) < max_length:\n","            seq = seq + [pad_token_id] * (max_length - len(seq))\n","        padded.append(seq[:max_length])\n","    return padded\n","\n","max_input_length = 512\n","max_target_length = 256\n","pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n","input_encodings = pad_sequences(input_encodings, max_input_length, pad_token_id)\n","target_encodings = pad_sequences(target_encodings, max_target_length, pad_token_id)\n","\n","# Convert to tensors\n","input_tensors = torch.tensor(input_encodings, dtype=torch.long)\n","target_tensors = torch.tensor(target_encodings, dtype=torch.long)\n","\n","# Split into train and validation (80-20)\n","train_size = int(0.8 * len(input_tensors))\n","train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n","train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n","\n","# Step 3: Define Transformer model from scratch\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","        src = self.embedding(src) * math.sqrt(self.d_model)\n","        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        tgt = self.pos_encoder(tgt)\n","        output = self.transformer(src, tgt, src_mask, tgt_mask)\n","        output = self.fc_out(output)\n","        return output\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","        return mask\n","\n","# Instantiate model\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(vocab_size=vocab_size).cuda()\n","\n","# Step 4: Training setup\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id, label_smoothing=0.1)\n","scaler = GradScaler()\n","scheduler = CosineAnnealingLR(optimizer, T_max=100)\n","\n","# Create dataset and dataloader\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs = inputs\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': self.inputs[idx], 'labels': self.targets[idx]}\n","\n","train_dataset = MedicalDataset(train_inputs, train_targets)\n","val_dataset = MedicalDataset(val_inputs, val_targets)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)\n","\n","# Step 5: Training loop\n","def train_epoch(model, loader, optimizer, criterion, scaler, scheduler):\n","    model.train()\n","    total_loss = 0\n","    for batch in loader:\n","        optimizer.zero_grad()\n","        src = batch['input_ids'].cuda()\n","        tgt = batch['labels'].cuda()\n","        tgt_input = tgt[:, :-1]\n","        tgt_output = tgt[:, 1:]\n","        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","        with autocast():\n","            output = model(src, tgt_input, tgt_mask=tgt_mask)\n","            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","    scheduler.step()\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in loader:\n","            src = batch['input_ids'].cuda()\n","            tgt = batch['labels'].cuda()\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","            with autocast():\n","                output = model(src, tgt_input, tgt_mask=tgt_mask)\n","                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(output, dim=-1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(tgt_output.cpu().numpy())\n","    return total_loss / len(loader), all_preds, all_labels\n","\n","# Step 6: Train the model\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, scheduler)\n","    val_loss, val_preds, val_labels = evaluate(model, val_loader, criterion)\n","\n","    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in val_preds]\n","    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in val_labels]\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    rouge_scores = [scorer.score(label, pred) for pred, label in zip(decoded_preds, decoded_labels)]\n","    rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n","\n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, ROUGE-1: {rouge1:.4f}\")\n","\n","# Step 7: Save the model\n","model_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(model_path, exist_ok=True)\n","torch.save(model.state_dict(), os.path.join(model_path, \"model.pt\"))\n","print(f\"Model saved to: {model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxsQWKUri6yD","outputId":"e90d7c5f-ca02-420d-cabb-c6b13f247e6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2986993457.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-2986993457.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","/tmp/ipython-input-2986993457.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 7.2693, Val Loss: 7.1810, ROUGE-1: 0.0000\n","Epoch 2, Train Loss: 7.0758, Val Loss: 7.1733, ROUGE-1: 0.0000\n","Epoch 3, Train Loss: 7.0671, Val Loss: 7.1746, ROUGE-1: 0.0000\n","Epoch 4, Train Loss: 7.0607, Val Loss: 7.1676, ROUGE-1: 0.0000\n","Epoch 5, Train Loss: 7.0545, Val Loss: 7.1583, ROUGE-1: 0.0000\n","Epoch 6, Train Loss: 7.0524, Val Loss: 7.1676, ROUGE-1: 0.0000\n","Epoch 7, Train Loss: 7.0482, Val Loss: 7.1510, ROUGE-1: 0.0000\n","Epoch 8, Train Loss: 7.0468, Val Loss: 7.1586, ROUGE-1: 0.0000\n","Epoch 9, Train Loss: 7.0473, Val Loss: 7.1486, ROUGE-1: 0.0000\n","Epoch 10, Train Loss: 7.0120, Val Loss: 7.1789, ROUGE-1: 0.0000\n","Epoch 11, Train Loss: 7.0109, Val Loss: 7.2699, ROUGE-1: 0.0000\n","Epoch 12, Train Loss: 6.9970, Val Loss: 7.2615, ROUGE-1: 0.0000\n","Epoch 13, Train Loss: 6.9787, Val Loss: 7.3851, ROUGE-1: 0.0000\n","Epoch 14, Train Loss: 6.9649, Val Loss: 7.4183, ROUGE-1: 0.0000\n","Epoch 15, Train Loss: 6.9592, Val Loss: 7.3545, ROUGE-1: 0.0000\n","Epoch 16, Train Loss: 6.9485, Val Loss: 7.5118, ROUGE-1: 0.0052\n","Epoch 17, Train Loss: 6.9371, Val Loss: 7.5578, ROUGE-1: 0.0052\n","Epoch 18, Train Loss: 6.9321, Val Loss: 7.6279, ROUGE-1: 0.0000\n","Epoch 19, Train Loss: 6.9258, Val Loss: 7.7429, ROUGE-1: 0.0052\n","Epoch 20, Train Loss: 6.9165, Val Loss: 7.8229, ROUGE-1: 0.0052\n","Epoch 21, Train Loss: 6.9093, Val Loss: 7.8903, ROUGE-1: 0.0052\n","Epoch 22, Train Loss: 6.9016, Val Loss: 7.9486, ROUGE-1: 0.0041\n","Epoch 23, Train Loss: 6.8994, Val Loss: 7.9220, ROUGE-1: 0.0052\n","Epoch 24, Train Loss: 6.8954, Val Loss: 7.9521, ROUGE-1: 0.0052\n","Epoch 25, Train Loss: 6.8916, Val Loss: 7.9099, ROUGE-1: 0.0041\n","Epoch 26, Train Loss: 6.8856, Val Loss: 7.9697, ROUGE-1: 0.0041\n","Epoch 27, Train Loss: 6.8827, Val Loss: 8.0157, ROUGE-1: 0.0052\n","Epoch 28, Train Loss: 6.8822, Val Loss: 8.0232, ROUGE-1: 0.0052\n","Epoch 29, Train Loss: 6.8768, Val Loss: 8.0445, ROUGE-1: 0.0041\n","Epoch 30, Train Loss: 6.8741, Val Loss: 8.0902, ROUGE-1: 0.0052\n","Epoch 31, Train Loss: 6.8690, Val Loss: 7.9912, ROUGE-1: 0.0052\n","Epoch 32, Train Loss: 6.8669, Val Loss: 8.0623, ROUGE-1: 0.0041\n","Epoch 33, Train Loss: 6.8685, Val Loss: 8.0553, ROUGE-1: 0.0052\n","Epoch 34, Train Loss: 6.8617, Val Loss: 8.0397, ROUGE-1: 0.0052\n","Epoch 35, Train Loss: 6.8574, Val Loss: 8.0630, ROUGE-1: 0.0041\n","Epoch 36, Train Loss: 6.8518, Val Loss: 8.0474, ROUGE-1: 0.0052\n","Epoch 37, Train Loss: 6.8520, Val Loss: 8.0669, ROUGE-1: 0.0052\n","Epoch 38, Train Loss: 6.8479, Val Loss: 8.0970, ROUGE-1: 0.0052\n","Epoch 39, Train Loss: 6.8514, Val Loss: 8.0551, ROUGE-1: 0.0041\n","Epoch 40, Train Loss: 6.8477, Val Loss: 8.0376, ROUGE-1: 0.0052\n","Epoch 41, Train Loss: 6.8426, Val Loss: 8.0696, ROUGE-1: 0.0041\n","Epoch 42, Train Loss: 6.8397, Val Loss: 8.0782, ROUGE-1: 0.0052\n","Epoch 43, Train Loss: 6.8346, Val Loss: 8.0523, ROUGE-1: 0.0041\n","Epoch 44, Train Loss: 6.8298, Val Loss: 8.0707, ROUGE-1: 0.0052\n","Epoch 45, Train Loss: 6.8302, Val Loss: 8.0558, ROUGE-1: 0.0052\n","Epoch 46, Train Loss: 6.8280, Val Loss: 8.0420, ROUGE-1: 0.0000\n","Epoch 47, Train Loss: 6.8234, Val Loss: 8.0722, ROUGE-1: 0.0052\n","Epoch 48, Train Loss: 6.8167, Val Loss: 8.0056, ROUGE-1: 0.0000\n","Epoch 49, Train Loss: 6.8136, Val Loss: 8.0615, ROUGE-1: 0.0041\n","Epoch 50, Train Loss: 6.8131, Val Loss: 8.0558, ROUGE-1: 0.0041\n","Epoch 51, Train Loss: 6.8085, Val Loss: 8.0404, ROUGE-1: 0.0000\n","Epoch 52, Train Loss: 6.8072, Val Loss: 8.0268, ROUGE-1: 0.0000\n","Epoch 53, Train Loss: 6.8046, Val Loss: 8.0184, ROUGE-1: 0.0000\n","Epoch 54, Train Loss: 6.8054, Val Loss: 7.9951, ROUGE-1: 0.0000\n","Epoch 55, Train Loss: 6.7949, Val Loss: 8.0115, ROUGE-1: 0.0000\n","Epoch 56, Train Loss: 6.7951, Val Loss: 8.0118, ROUGE-1: 0.0000\n","Epoch 57, Train Loss: 6.7906, Val Loss: 8.0087, ROUGE-1: 0.0000\n","Epoch 58, Train Loss: 6.7874, Val Loss: 7.9988, ROUGE-1: 0.0000\n","Epoch 59, Train Loss: 6.7843, Val Loss: 7.9872, ROUGE-1: 0.0000\n","Epoch 60, Train Loss: 6.7826, Val Loss: 7.9947, ROUGE-1: 0.0000\n","Epoch 61, Train Loss: 6.7799, Val Loss: 8.0073, ROUGE-1: 0.0000\n","Epoch 62, Train Loss: 6.7793, Val Loss: 8.0021, ROUGE-1: 0.0000\n","Epoch 63, Train Loss: 6.7758, Val Loss: 8.0230, ROUGE-1: 0.0041\n","Epoch 64, Train Loss: 6.7720, Val Loss: 8.0265, ROUGE-1: 0.0000\n","Epoch 65, Train Loss: 6.7725, Val Loss: 8.0104, ROUGE-1: 0.0000\n","Epoch 66, Train Loss: 6.7699, Val Loss: 8.0152, ROUGE-1: 0.0000\n","Epoch 67, Train Loss: 6.7680, Val Loss: 7.9990, ROUGE-1: 0.0000\n","Epoch 68, Train Loss: 6.7646, Val Loss: 8.0313, ROUGE-1: 0.0000\n","Epoch 69, Train Loss: 6.7666, Val Loss: 8.0231, ROUGE-1: 0.0000\n","Epoch 70, Train Loss: 6.7614, Val Loss: 8.0300, ROUGE-1: 0.0000\n","Epoch 71, Train Loss: 6.7651, Val Loss: 8.0074, ROUGE-1: 0.0000\n","Epoch 72, Train Loss: 6.7569, Val Loss: 8.0337, ROUGE-1: 0.0000\n","Epoch 73, Train Loss: 6.7560, Val Loss: 8.0320, ROUGE-1: 0.0000\n","Epoch 74, Train Loss: 6.7564, Val Loss: 8.0378, ROUGE-1: 0.0000\n","Epoch 75, Train Loss: 6.7516, Val Loss: 8.0422, ROUGE-1: 0.0000\n","Epoch 76, Train Loss: 6.7492, Val Loss: 8.0476, ROUGE-1: 0.0000\n","Epoch 77, Train Loss: 6.7522, Val Loss: 8.0217, ROUGE-1: 0.0000\n","Epoch 78, Train Loss: 6.7428, Val Loss: 8.0261, ROUGE-1: 0.0000\n","Epoch 79, Train Loss: 6.7437, Val Loss: 8.0415, ROUGE-1: 0.0000\n","Epoch 80, Train Loss: 6.7463, Val Loss: 8.0337, ROUGE-1: 0.0000\n","Epoch 81, Train Loss: 6.7431, Val Loss: 8.0647, ROUGE-1: 0.0000\n","Epoch 82, Train Loss: 6.7429, Val Loss: 8.0545, ROUGE-1: 0.0000\n","Epoch 83, Train Loss: 6.7403, Val Loss: 8.0571, ROUGE-1: 0.0000\n","Epoch 84, Train Loss: 6.7409, Val Loss: 8.0545, ROUGE-1: 0.0000\n","Epoch 85, Train Loss: 6.7358, Val Loss: 8.0421, ROUGE-1: 0.0000\n","Epoch 86, Train Loss: 6.7385, Val Loss: 8.0364, ROUGE-1: 0.0000\n","Epoch 87, Train Loss: 6.7363, Val Loss: 8.0456, ROUGE-1: 0.0000\n","Epoch 88, Train Loss: 6.7315, Val Loss: 8.0500, ROUGE-1: 0.0000\n","Epoch 89, Train Loss: 6.7279, Val Loss: 8.0542, ROUGE-1: 0.0000\n","Epoch 90, Train Loss: 6.7322, Val Loss: 8.0537, ROUGE-1: 0.0000\n","Epoch 91, Train Loss: 6.7306, Val Loss: 8.0612, ROUGE-1: 0.0000\n","Epoch 92, Train Loss: 6.7318, Val Loss: 8.0514, ROUGE-1: 0.0000\n","Epoch 93, Train Loss: 6.7328, Val Loss: 8.0636, ROUGE-1: 0.0000\n","Epoch 94, Train Loss: 6.7338, Val Loss: 8.0580, ROUGE-1: 0.0000\n","Epoch 95, Train Loss: 6.7284, Val Loss: 8.0594, ROUGE-1: 0.0000\n","Epoch 96, Train Loss: 6.7279, Val Loss: 8.0591, ROUGE-1: 0.0000\n","Epoch 97, Train Loss: 6.7270, Val Loss: 8.0599, ROUGE-1: 0.0000\n","Epoch 98, Train Loss: 6.7291, Val Loss: 8.0596, ROUGE-1: 0.0000\n","Epoch 99, Train Loss: 6.7315, Val Loss: 8.0608, ROUGE-1: 0.0000\n","Epoch 100, Train Loss: 6.7298, Val Loss: 8.0611, ROUGE-1: 0.0000\n","Model saved to: /content/drive/MyDrive/Transformer\n"]}]},{"cell_type":"markdown","source":["# Trial 4"],"metadata":{"id":"eL0Mq75-_I2h"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import math\n","import os\n","from rouge_score import rouge_scorer\n","\n","# =================================== 1. Load data ===================================\n","df = pd.read_csv('/content/drive/MyDrive/Transformer/mtsamples.csv')\n","df = df[['transcription', 'description']].dropna()\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","print(f\"Loaded {len(df)} samples\")\n","\n","# =================================== 2. Train tokenizer ===================================\n","texts = list(df['text']) + list(df['summary'])\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","tokenizer.train_from_iterator(texts, trainer)\n","\n","save_dir = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(save_dir, exist_ok=True)\n","tokenizer.save(os.path.join(save_dir, \"tokenizer4.json\"))\n","\n","CLS_ID = tokenizer.token_to_id(\"[CLS]\")\n","SEP_ID = tokenizer.token_to_id(\"[SEP]\")\n","PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n","print(f\"Special tokens → CLS: {CLS_ID}, SEP: {SEP_ID}, PAD: {PAD_ID}\")\n","\n","# =================================== 3. CORRECT ENCODING (THIS IS THE FIX) ===================================\n","def encode_pair(text, summary):\n","    src = tokenizer.encode(text).ids[:512]\n","    tgt_ids = tokenizer.encode(summary).ids\n","    tgt = [CLS_ID] + tgt_ids + [SEP_ID]           # BOS + summary + EOS\n","    tgt = tgt[:256]\n","    return src, tgt\n","\n","inputs, targets = [], []\n","for text, summary in zip(df['text'], df['summary']):\n","    src, tgt = encode_pair(text, summary)\n","    inputs.append(src + [PAD_ID] * (512 - len(src)))\n","    targets.append(tgt + [PAD_ID] * (256 - len(tgt)))\n","\n","inputs = torch.tensor(inputs, dtype=torch.long)\n","targets = torch.tensor(targets, dtype=torch.long)\n","\n","# Train/val split\n","train_size = int(0.8 * len(inputs))\n","train_in, val_in = inputs[:train_size], inputs[train_size:]\n","train_tgt, val_tgt = targets[:train_size], targets[train_size:]\n","\n","# =================================== 4. Model (same as Streamlit) ===================================\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","    def forward(self, x): return x + self.pe[:, :x.size(1)]\n","\n","class TransformerSummarizer(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pos = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n","                                          num_encoder_layers=num_layers,\n","                                          num_decoder_layers=num_layers,\n","                                          dim_feedforward=2048, dropout=0.1,\n","                                          batch_first=True)\n","        self.out = nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, tgt_mask=None):\n","        src = self.embed(src) * math.sqrt(self.d_model)\n","        tgt = self.embed(tgt) * math.sqrt(self.d_model)\n","        src = self.pos(src)\n","        tgt = self.pos(tgt)\n","        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n","        return self.out(out)\n","\n","    def generate_mask(self, sz):\n","        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","\n","model = TransformerSummarizer(vocab_size=tokenizer.get_vocab_size()).cuda()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n","scaler = GradScaler()\n","\n","# =================================== 5. Training loop ===================================\n","@torch.no_grad()\n","def validate():\n","    model.eval()\n","    total_loss = 0\n","    scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'])\n","    r1 = []\n","    for i in range(0, len(val_in), 4):\n","        src = val_in[i:i+4].cuda()\n","        tgt = val_tgt[i:i+4].cuda()\n","        tgt_in = tgt[:, :-1]\n","        with autocast():\n","            out = model(src, tgt_in, model.generate_mask(tgt_in.size(1)).cuda())\n","            loss = criterion(out.reshape(-1, out.size(-1)), tgt[:, 1:].reshape(-1))\n","        total_loss += loss.item()\n","        pred = out.argmax(-1)\n","        for p, t in zip(pred, tgt):\n","            r1.append(scorer.score(tokenizer.decode(t[1:].tolist(), skip_special_tokens=True),\n","                                 tokenizer.decode(p.tolist(), skip_special_tokens=True))['rouge1'].fmeasure)\n","    return total_loss / (len(val_in)//4), sum(r1)/len(r1)\n","\n","for epoch in range(1, 31):\n","    model.train()\n","    total = 0\n","    for i in range(0, len(train_in), 4):\n","        optimizer.zero_grad()\n","        src = train_in[i:i+4].cuda()\n","        tgt = train_tgt[i:i+4].cuda()\n","        tgt_in = tgt[:, :-1]\n","        mask = model.generate_mask(tgt_in.size(1)).cuda()\n","        with autocast():\n","            out = model(src, tgt_in, mask)\n","            loss = criterion(out.reshape(-1, out.size(-1)), tgt[:, 1:].reshape(-1))\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total += loss.item()\n","    val_loss, rouge1 = validate()\n","    print(f\"Epoch {epoch:3d} | Train Loss: {total/(len(train_in)//4):.4f} | Val Loss: {val_loss:.4f} | ROUGE-1: {rouge1:.4f}\")\n","\n","# =================================== 6. Save ===================================\n","torch.save(model.state_dict(), os.path.join(save_dir, \"model4.pt\"))\n","print(\"FINAL MODEL & TOKENIZER SAVED — READY FOR STREAMLIT\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yU4tOi1__AoN","outputId":"07d423c0-b30b-47b1-83e3-09dec9a584ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 4966 samples\n","Special tokens → CLS: 2, SEP: 3, PAD: 0\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1984083123.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-1984083123.py:129: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","/tmp/ipython-input-1984083123.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch   1 | Train Loss: 7.2881 | Val Loss: 7.0654 | ROUGE-1: 0.0793\n","Epoch   2 | Train Loss: 6.8368 | Val Loss: 7.0387 | ROUGE-1: 0.0645\n","Epoch   3 | Train Loss: 6.7462 | Val Loss: 7.0307 | ROUGE-1: 0.0641\n","Epoch   4 | Train Loss: 6.8028 | Val Loss: 7.0553 | ROUGE-1: 0.0350\n","Epoch   5 | Train Loss: 6.6807 | Val Loss: 7.0373 | ROUGE-1: 0.0752\n","Epoch   6 | Train Loss: 6.6993 | Val Loss: 7.2443 | ROUGE-1: 0.0342\n","Epoch   7 | Train Loss: 6.8457 | Val Loss: 7.2347 | ROUGE-1: 0.0617\n","Epoch   8 | Train Loss: 6.8278 | Val Loss: 7.2899 | ROUGE-1: 0.0350\n","Epoch   9 | Train Loss: 6.8518 | Val Loss: 7.2959 | ROUGE-1: 0.0767\n","Epoch  10 | Train Loss: 6.8469 | Val Loss: 7.4409 | ROUGE-1: 0.0000\n","Epoch  11 | Train Loss: 6.9917 | Val Loss: 7.4249 | ROUGE-1: 0.0000\n","Epoch  12 | Train Loss: 6.9918 | Val Loss: 7.5114 | ROUGE-1: 0.0000\n","Epoch  13 | Train Loss: 6.9680 | Val Loss: 7.4982 | ROUGE-1: 0.0000\n","Epoch  14 | Train Loss: 6.9241 | Val Loss: 7.5216 | ROUGE-1: 0.0000\n","Epoch  15 | Train Loss: 6.9311 | Val Loss: 7.5263 | ROUGE-1: 0.0000\n","Epoch  16 | Train Loss: 6.9328 | Val Loss: 7.5194 | ROUGE-1: 0.0000\n","Epoch  17 | Train Loss: 6.9679 | Val Loss: 7.4967 | ROUGE-1: 0.0000\n","Epoch  18 | Train Loss: 6.9574 | Val Loss: 7.4885 | ROUGE-1: 0.0000\n","Epoch  19 | Train Loss: 6.9728 | Val Loss: 7.6370 | ROUGE-1: 0.0041\n","Epoch  20 | Train Loss: 6.9520 | Val Loss: 7.7197 | ROUGE-1: 0.0041\n","Epoch  21 | Train Loss: 6.9281 | Val Loss: 7.5948 | ROUGE-1: 0.0000\n","Epoch  22 | Train Loss: 6.9677 | Val Loss: 7.6151 | ROUGE-1: 0.0000\n","Epoch  23 | Train Loss: 6.9523 | Val Loss: 7.6312 | ROUGE-1: 0.0000\n","Epoch  24 | Train Loss: 6.9322 | Val Loss: 7.6075 | ROUGE-1: 0.0000\n","Epoch  25 | Train Loss: 6.9440 | Val Loss: 7.8314 | ROUGE-1: 0.0000\n","Epoch  26 | Train Loss: 6.9112 | Val Loss: 7.7201 | ROUGE-1: 0.0000\n","Epoch  27 | Train Loss: 6.9189 | Val Loss: 7.9613 | ROUGE-1: 0.0041\n","Epoch  28 | Train Loss: 6.9213 | Val Loss: 8.2290 | ROUGE-1: 0.0041\n","Epoch  29 | Train Loss: 6.9407 | Val Loss: 8.1125 | ROUGE-1: 0.0026\n","Epoch  30 | Train Loss: 6.9787 | Val Loss: 7.9272 | ROUGE-1: 0.0000\n","FINAL MODEL & TOKENIZER SAVED — READY FOR STREAMLIT\n"]}]},{"cell_type":"markdown","source":["# Greedy Seach"],"metadata":{"id":"Dy8dVuHgEjun"}},{"cell_type":"code","source":["# FINAL TRAINING NOTEBOOK — 100% WORKING (syntax fixed)\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import math\n","import os\n","from rouge_score import rouge_scorer\n","\n","# ====================== 1. Load Data ======================\n","df = pd.read_csv('/content/drive/MyDrive/Transformer/mtsamples.csv')\n","df = df[['transcription', 'description']].dropna()\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","print(f\"Loaded {len(df)} samples\")\n","\n","# ====================== 2. Train Tokenizer ======================\n","texts = list(df['text']) + list(df['summary'])\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","tokenizer.train_from_iterator(texts, trainer)\n","\n","save_dir = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(save_dir, exist_ok=True)\n","tokenizer.save(os.path.join(save_dir, \"tokenizer5.json\"))\n","\n","CLS_ID = tokenizer.token_to_id(\"[CLS]\")\n","SEP_ID = tokenizer.token_to_id(\"[SEP]\")\n","PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n","print(f\"Special tokens → [CLS]: {CLS_ID}, [SEP]: {SEP_ID}, [PAD]: {PAD_ID}\")   # ← Fixed!\n","\n","# ====================== 3. CORRECT ENCODING (THIS IS THE KEY FIX) ======================\n","def encode_pair(text, summary):\n","    src = tokenizer.encode(text).ids[:512]\n","    tgt_ids = tokenizer.encode(summary).ids\n","    tgt = [CLS_ID] + tgt_ids + [SEP_ID]          # BOS + summary + EOS\n","    tgt = tgt[:256]\n","    src += [PAD_ID] * (512 - len(src))\n","    tgt += [PAD_ID] * (256 - len(tgt))\n","    return src, tgt\n","\n","inputs, targets = zip(*[encode_pair(t, s) for t, s in zip(df['text'], df['summary'])])\n","inputs = torch.tensor(inputs, dtype=torch.long)\n","targets = torch.tensor(targets, dtype=torch.long)\n","\n","# Train/val split\n","train_size = int(0.8 * len(inputs))\n","train_in, val_in = inputs[:train_size], inputs[train_size:]\n","train_tgt, val_tgt = targets[:train_size], targets[train_size:]\n","\n","# ====================== 4. Model ======================\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        d_model = 512\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pos = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(d_model=d_model, nhead=8,\n","                                          num_encoder_layers=6, num_decoder_layers=6,\n","                                          dim_feedforward=2048, dropout=0.1,\n","                                          batch_first=True)\n","        self.out = nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, tgt_mask=None):\n","        src = self.embed(src) * math.sqrt(self.d_model)\n","        tgt = self.embed(tgt) * math.sqrt(self.d_model)\n","        src = self.pos(src)\n","        tgt = self.pos(tgt)\n","        return self.out(self.transformer(src, tgt, tgt_mask=tgt_mask))\n","\n","    def generate_mask(self, sz):\n","        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","\n","# ====================== 5. Training ======================\n","model = TransformerModel(vocab_size=tokenizer.get_vocab_size()).cuda()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n","scaler = GradScaler()\n","\n","print(\"Starting training...\")\n","for epoch in range(1, 41):\n","    model.train()\n","    total = 0.0\n","    for i in range(0, len(train_in), 4):\n","        optimizer.zero_grad()\n","        src = train_in[i:i+4].cuda()\n","        tgt = train_tgt[i:i+4].cuda()\n","        tgt_in = tgt[:, :-1]\n","        mask = model.generate_mask(tgt_in.size(1)).cuda()\n","\n","        with autocast():\n","            out = model(src, tgt_in, mask)\n","            loss = criterion(out.reshape(-1, out.size(-1)), tgt[:, 1:].reshape(-1))\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total += loss.item()\n","\n","    # Quick validation ROUGE-1\n","    model.eval()\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])\n","    r1_scores = []\n","    with torch.no_grad():\n","        for i in range(0, len(val_in), 4):\n","            src = val_in[i:i+4].cuda()\n","            tgt = val_tgt[i:i+4].cuda()\n","            tgt_in = tgt[:, :-1]\n","            out = model(src, tgt_in, model.generate_mask(tgt_in.size(1)).cuda())\n","            pred = out.argmax(-1)\n","            for p, t in zip(pred, tgt):\n","                ref = tokenizer.decode(t[1:].tolist(), skip_special_tokens=True)\n","                gen = tokenizer.decode(p.tolist(), skip_special_tokens=True)\n","                r1_scores.append(scorer.score(ref, gen)['rouge1'].fmeasure)\n","\n","    rouge1 = sum(r1_scores) / len(r1_scores) if r1_scores else 0\n","    print(f\"Epoch {epoch:3d} | TrainLoss {total/(len(train_in)//4):.4f} | ValROUGE-1 {rouge1:.4f}\")\n","\n","# ====================== 6. Save ======================\n","torch.save(model.state_dict(), os.path.join(save_dir, \"model5.pt\"))\n","print(\"Training finished! model.pt and tokenizer.json saved to /content/drive/MyDrive/Transformer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RzDnayaEoAS","outputId":"ae75f46c-6774-4546-ece2-fcdb3dd71c72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 4966 samples\n","Special tokens → [CLS]: 2, [SEP]: 3, [PAD]: 0\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2557971320.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-2557971320.py:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Epoch   1 | TrainLoss 7.2776 | ValROUGE-1 0.0353\n","Epoch   2 | TrainLoss 6.8599 | ValROUGE-1 0.0000\n","Epoch   3 | TrainLoss 6.8630 | ValROUGE-1 0.0768\n","Epoch   4 | TrainLoss 6.8787 | ValROUGE-1 0.0000\n","Epoch   5 | TrainLoss 6.9434 | ValROUGE-1 0.0000\n","Epoch   6 | TrainLoss 7.0075 | ValROUGE-1 0.0000\n","Epoch   7 | TrainLoss 6.9683 | ValROUGE-1 0.0000\n","Epoch   8 | TrainLoss 6.9790 | ValROUGE-1 0.0000\n","Epoch   9 | TrainLoss 6.9446 | ValROUGE-1 0.0000\n","Epoch  10 | TrainLoss 6.9363 | ValROUGE-1 0.0000\n","Epoch  11 | TrainLoss 6.9261 | ValROUGE-1 0.0000\n","Epoch  12 | TrainLoss 6.9265 | ValROUGE-1 0.0000\n","Epoch  13 | TrainLoss 6.9385 | ValROUGE-1 0.0000\n","Epoch  14 | TrainLoss 7.0194 | ValROUGE-1 0.0000\n","Epoch  15 | TrainLoss 7.1193 | ValROUGE-1 0.0000\n","Epoch  16 | TrainLoss 7.1418 | ValROUGE-1 0.0000\n","Epoch  17 | TrainLoss 7.0998 | ValROUGE-1 0.0000\n","Epoch  18 | TrainLoss 7.0870 | ValROUGE-1 0.0000\n","Epoch  19 | TrainLoss 7.1111 | ValROUGE-1 0.0000\n","Epoch  20 | TrainLoss 7.0882 | ValROUGE-1 0.0000\n","Epoch  21 | TrainLoss 7.0496 | ValROUGE-1 0.0000\n","Epoch  22 | TrainLoss 7.0517 | ValROUGE-1 0.0000\n","Epoch  23 | TrainLoss 7.0385 | ValROUGE-1 0.0000\n","Epoch  24 | TrainLoss 7.0073 | ValROUGE-1 0.0000\n","Epoch  25 | TrainLoss 7.0126 | ValROUGE-1 0.0000\n","Epoch  26 | TrainLoss 7.0526 | ValROUGE-1 0.0000\n","Epoch  27 | TrainLoss 7.0739 | ValROUGE-1 0.0000\n","Epoch  28 | TrainLoss 7.0122 | ValROUGE-1 0.0000\n","Epoch  29 | TrainLoss 6.9966 | ValROUGE-1 0.0000\n","Epoch  30 | TrainLoss 6.9979 | ValROUGE-1 0.0000\n","Epoch  31 | TrainLoss 6.9969 | ValROUGE-1 0.0000\n","Epoch  32 | TrainLoss 6.9760 | ValROUGE-1 0.0000\n","Epoch  33 | TrainLoss 6.9743 | ValROUGE-1 0.0000\n","Epoch  34 | TrainLoss 6.9896 | ValROUGE-1 0.0000\n","Epoch  35 | TrainLoss 7.0408 | ValROUGE-1 0.0000\n","Epoch  36 | TrainLoss 7.0159 | ValROUGE-1 0.0000\n","Epoch  37 | TrainLoss 7.0673 | ValROUGE-1 0.0000\n","Epoch  38 | TrainLoss 7.0672 | ValROUGE-1 0.0000\n","Epoch  39 | TrainLoss 7.0705 | ValROUGE-1 0.0000\n","Epoch  40 | TrainLoss 7.0196 | ValROUGE-1 0.0000\n","Training finished! model.pt and tokenizer.json saved to /content/drive/MyDrive/Transformer\n"]}]},{"cell_type":"markdown","source":["# Test 6"],"metadata":{"id":"OITZiej6kyyc"}},{"cell_type":"code","source":["# training_colab.py — Colab-ready\n","import os\n","import math\n","import time\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","from rouge_score import rouge_scorer\n","\n","# ========== CONFIG ==========\n","DATA_CSV = \"/content/drive/MyDrive/Transformer/mtsamples.csv\"   # update if needed\n","SAVE_DIR = \"/content/drive/MyDrive/Transformer\"\n","TOKENIZER_NAME = \"tokenizer6.json\"\n","MODEL_NAME = \"model6.pt\"\n","\n","VOCAB_SIZE = 30000\n","MAX_SRC_LEN = 512\n","MAX_TGT_LEN = 256\n","BATCH_SIZE = 4\n","NUM_EPOCHS = 100\n","LR = 1e-4\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","print(f\"[config] DEVICE={DEVICE} SAVE_DIR={SAVE_DIR}\")\n","\n","# ========== 1. Load data ==========\n","if not os.path.exists(DATA_CSV):\n","    raise FileNotFoundError(f\"Data CSV not found at {DATA_CSV}. Upload to Drive or change DATA_CSV.\")\n","df = pd.read_csv(DATA_CSV)\n","df = df[['transcription', 'description']].dropna()\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","print(f\"[data] loaded {len(df)} rows. Examples:\")\n","print(df.head(2).to_dict(orient='records'))\n","\n","# ========== 2. Train tokenizer (WordPiece) ==========\n","special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n","\n","print(\"[tokenizer] training... (this may take a while)\")\n","# Use generator to avoid building a giant intermediate list in memory\n","def iter_texts():\n","    for t,s in zip(df['text'].astype(str), df['summary'].astype(str)):\n","        yield t\n","        yield s\n","\n","tokenizer.train_from_iterator(iter_texts(), trainer=trainer)\n","tokenizer_path = os.path.join(SAVE_DIR, TOKENIZER_NAME)\n","tokenizer.save(tokenizer_path)\n","print(f\"[tokenizer] saved to {tokenizer_path}\")\n","\n","# Check special token ids exist and are valid ints\n","CLS_ID = tokenizer.token_to_id(\"[CLS]\")\n","SEP_ID = tokenizer.token_to_id(\"[SEP]\")\n","PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n","UNK_ID = tokenizer.token_to_id(\"[UNK]\")\n","\n","if any(x is None for x in (CLS_ID, SEP_ID, PAD_ID, UNK_ID)):\n","    print(\"ERROR: One or more special tokens missing after training. Tokenizer vocab size:\", tokenizer.get_vocab_size())\n","    print(\"Token ids:\", {\"CLS\":CLS_ID,\"SEP\":SEP_ID,\"PAD\":PAD_ID,\"UNK\":UNK_ID})\n","    raise SystemExit(\"Special tokens not present. Re-run tokenizer training with special_tokens configured.\")\n","\n","print(f\"[tokenizer] ids -> CLS:{CLS_ID} SEP:{SEP_ID} PAD:{PAD_ID} UNK:{UNK_ID}  vocab_size={tokenizer.get_vocab_size()}\")\n","\n","# Quick tokenization sanity-check on a small example\n","sample_text = df['text'].iloc[0]\n","sample_summary = df['summary'].iloc[0]\n","print(\"[sanity] sample text (truncated):\", sample_text[:200])\n","print(\"[sanity] sample summary (truncated):\", sample_summary[:200])\n","enc_src = tokenizer.encode(sample_text).ids[:50]\n","enc_tgt = tokenizer.encode(sample_summary).ids[:50]\n","print(\"[sanity] encoded src ids (first 50):\", enc_src[:50])\n","print(\"[sanity] encoded tgt ids (first 50):\", enc_tgt[:50])\n","# Decoding the encoded tokens back\n","print(\"[sanity] decode src:\", tokenizer.decode(enc_src, skip_special_tokens=True)[:200])\n","print(\"[sanity] decode tgt:\", tokenizer.decode(enc_tgt, skip_special_tokens=True)[:200])\n","\n","# ========== 3. Encoding helper ==========\n","def encode_pair(text: str, summary: str):\n","    src_ids = tokenizer.encode(str(text)).ids[:MAX_SRC_LEN]\n","    tgt_ids = tokenizer.encode(str(summary)).ids\n","    tgt = [CLS_ID] + tgt_ids + [SEP_ID]\n","    src_ids = src_ids[:MAX_SRC_LEN]\n","    tgt = tgt[:MAX_TGT_LEN]\n","    src_ids += [PAD_ID] * (MAX_SRC_LEN - len(src_ids))\n","    tgt += [PAD_ID] * (MAX_TGT_LEN - len(tgt))\n","    return src_ids, tgt\n","\n","print(\"[encode] building tensors (this may take a moment)...\")\n","pairs = [encode_pair(t, s) for t, s in zip(df['text'], df['summary'])]\n","srcs, tgts = zip(*pairs)\n","src_tensor = torch.tensor(srcs, dtype=torch.long)\n","tgt_tensor = torch.tensor(tgts, dtype=torch.long)\n","print(\"[encode] done. shapes:\", src_tensor.shape, tgt_tensor.shape)\n","\n","# train/val split\n","train_size = int(0.8 * len(src_tensor))\n","train_src, val_src = src_tensor[:train_size], src_tensor[train_size:]\n","train_tgt, val_tgt = tgt_tensor[:train_size], tgt_tensor[train_size:]\n","print(f\"[split] train {len(train_src)} val {len(val_src)}\")\n","\n","# ========== 4. Model ==========\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, nhead=8,\n","                 num_encoder_layers=6, num_decoder_layers=6,\n","                 dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n","        self.pos = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n","                                          num_encoder_layers=num_encoder_layers,\n","                                          num_decoder_layers=num_decoder_layers,\n","                                          dim_feedforward=dim_feedforward,\n","                                          dropout=dropout,\n","                                          batch_first=True)\n","        self.out = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, src, tgt, tgt_mask=None):\n","        src_emb = self.embed(src) * math.sqrt(self.d_model)\n","        tgt_emb = self.embed(tgt) * math.sqrt(self.d_model)\n","        src_emb = self.pos(src_emb)\n","        tgt_emb = self.pos(tgt_emb)\n","        out = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n","        logits = self.out(out)\n","        return logits\n","\n","    @staticmethod\n","    def generate_mask(sz):\n","        return torch.triu(torch.ones(sz, sz, device=DEVICE) * float('-inf'), diagonal=1)\n","\n","# instantiate and move to device\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(vocab_size=vocab_size).to(DEVICE)\n","print(f\"[model] vocab_size={vocab_size} device={DEVICE}\")\n","\n","# ========== 5. Training utilities ==========\n","optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n","scaler = GradScaler()\n","\n","class SimpleDataset(torch.utils.data.Dataset):\n","    def __init__(self, srcs, tgts):\n","        self.srcs = srcs\n","        self.tgts = tgts\n","    def __len__(self):\n","        return len(self.srcs)\n","    def __getitem__(self, idx):\n","        return {'src': self.srcs[idx], 'tgt': self.tgts[idx]}\n","\n","train_ds = SimpleDataset(train_src, train_tgt)\n","val_ds = SimpleDataset(val_src, val_tgt)\n","train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=2)\n","\n","# ========== 6. Training loop ==========\n","print(\"[train] starting training...\")\n","start_time = time.time()\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    model.train()\n","    train_loss = 0.0\n","    num_batches = 0\n","    for batch in train_loader:\n","        optimizer.zero_grad()\n","        src = batch['src'].to(DEVICE)\n","        tgt = batch['tgt'].to(DEVICE)\n","        tgt_in = tgt[:, :-1]\n","        tgt_out = tgt[:, 1:]\n","\n","        tgt_mask = model.generate_mask(tgt_in.size(1)).to(DEVICE)\n","\n","        with autocast():\n","            logits = model(src, tgt_in, tgt_mask=tgt_mask)\n","            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        train_loss += loss.item()\n","        num_batches += 1\n","\n","    avg_train_loss = train_loss / max(1, num_batches)\n","\n","    # quick validation - compute ROUGE-1 on greedy pred\n","    model.eval()\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n","    r1_scores = []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            src = batch['src'].to(DEVICE)\n","            tgt = batch['tgt'].to(DEVICE)\n","            tgt_in = tgt[:, :-1]\n","            tgt_mask = model.generate_mask(tgt_in.size(1)).to(DEVICE)\n","            logits = model(src, tgt_in, tgt_mask=tgt_mask)\n","            preds = logits.argmax(dim=-1)\n","\n","            for p, t in zip(preds, tgt):\n","                ref_ids = t[1:].tolist()\n","                if SEP_ID in ref_ids:\n","                    ref_ids = ref_ids[:ref_ids.index(SEP_ID)]\n","                ref_ids = [x for x in ref_ids if x != PAD_ID]\n","                gen_ids = p.tolist()\n","                if SEP_ID in gen_ids:\n","                    gen_ids = gen_ids[:gen_ids.index(SEP_ID)]\n","                # decode\n","                ref_text = tokenizer.decode(ref_ids, skip_special_tokens=True).strip()\n","                gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n","                if ref_text and gen_text:\n","                    r1_scores.append(scorer.score(ref_text, gen_text)['rouge1'].fmeasure)\n","\n","    val_rouge1 = sum(r1_scores) / len(r1_scores) if r1_scores else 0.0\n","    elapsed = time.time() - start_time\n","    print(f\"Epoch {epoch:3d} | TrainLoss {avg_train_loss:.4f} | ValROUGE-1 {val_rouge1:.4f} | elapsed {elapsed/60:.1f} min\")\n","\n","# ========== 7. Save ==========\n","torch.save(model.state_dict(), os.path.join(SAVE_DIR, MODEL_NAME))\n","print(f\"[save] model saved to {os.path.join(SAVE_DIR, MODEL_NAME)}\")\n","print(\"Training finished.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOkIHiZjk2j5","outputId":"83db2791-8da5-48ea-e6f7-82b013dfee36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[config] DEVICE=cuda SAVE_DIR=/content/drive/MyDrive/Transformer\n","[data] loaded 4966 rows. Examples:\n","[{'text': 'SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.', 'summary': ' A 23-year-old white female presents with complaint of allergies.'}, {'text': 'PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off the floor.  He exercises three times a week at home and does cardio.  He has difficulty walking two blocks or five flights of stairs.  Difficulty with snoring.  He has muscle and joint pains including knee pain, back pain, foot and ankle pain, and swelling.  He has gastroesophageal reflux disease.,PAST SURGICAL HISTORY:, Includes reconstructive surgery on his right hand 13 years ago.  ,SOCIAL HISTORY:, He is currently single.  He has about ten drinks a year.  He had smoked significantly up until several months ago.  He now smokes less than three cigarettes a day.,FAMILY HISTORY:, Heart disease in both grandfathers, grandmother with stroke, and a grandmother with diabetes.  Denies obesity and hypertension in other family members.,CURRENT MEDICATIONS:, None.,ALLERGIES:,  He is allergic to Penicillin.,MISCELLANEOUS/EATING HISTORY:, He has been going to support groups for seven months with Lynn Holmberg in Greenwich and he is from Eastchester, New York and he feels that we are the appropriate program.  He had a poor experience with the Greenwich program.  Eating history, he is not an emotional eater.  Does not like sweets.  He likes big portions and carbohydrates.  He likes chicken and not steak.  He currently weighs 312 pounds.  Ideal body weight would be 170 pounds.  He is 142 pounds overweight.  If ,he lost 60% of his excess body weight that would be 84 pounds and he should weigh about 228.,REVIEW OF SYSTEMS: ,Negative for head, neck, heart, lungs, GI, GU, orthopedic, and skin.  Specifically denies chest pain, heart attack, coronary artery disease, congestive heart failure, arrhythmia, atrial fibrillation, pacemaker, high cholesterol, pulmonary embolism, high blood pressure, CVA, venous insufficiency, thrombophlebitis, asthma, shortness of breath, COPD, emphysema, sleep apnea, diabetes, leg and foot swelling, osteoarthritis, rheumatoid arthritis, hiatal hernia, peptic ulcer disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, hemorrhoids, rectal bleeding, polyps, incontinence of stool, urinary stress incontinence, or cancer.  Denies cellulitis, pseudotumor cerebri, meningitis, or encephalitis.,PHYSICAL EXAMINATION:, He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Afebrile.  Vital Signs are stable.', 'summary': ' Consult for laparoscopic gastric bypass.'}]\n","[tokenizer] training... (this may take a while)\n","[tokenizer] saved to /content/drive/MyDrive/Transformer/tokenizer6.json\n","[tokenizer] ids -> CLS:2 SEP:3 PAD:0 UNK:1  vocab_size=30000\n","[sanity] sample text (truncated): SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried \n","[sanity] sample summary (truncated):  A 23-year-old white female presents with complaint of allergies.\n","[sanity] encoded src ids (first 50): [3848, 287, 493, 2988, 17, 451, 17, 547, 1447, 970, 2093, 229, 1923, 219, 2538, 18, 324, 593, 220, 480, 2538, 1296, 368, 10350, 214, 22764, 501, 368, 7359, 1817, 341, 3198, 2390, 18, 694, 203, 1194, 16, 368, 315, 5067, 9046, 16, 211, 8772, 18, 4005, 4779, 263, 1305]\n","[sanity] encoded tgt ids (first 50): [37, 2988, 17, 451, 17, 547, 1447, 970, 2093, 229, 1923, 219, 2538, 18]\n","[sanity] decode src: SUBJECTIVE :, This 23 - year - old white female presents with complaint of allergies . She used to have allergies when she lived in Seattle but she thinks they are worse here . In the past , she has t\n","[sanity] decode tgt: A 23 - year - old white female presents with complaint of allergies .\n","[encode] building tensors (this may take a moment)...\n","[encode] done. shapes: torch.Size([4966, 512]) torch.Size([4966, 256])\n","[split] train 3972 val 994\n","[model] vocab_size=30000 device=cuda\n","[train] starting training...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3203531830.py:161: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-3203531830.py:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch   1 | TrainLoss 7.1215 | ValROUGE-1 0.1288 | elapsed 1.8 min\n","Epoch   2 | TrainLoss 6.3297 | ValROUGE-1 0.1706 | elapsed 3.7 min\n","Epoch   3 | TrainLoss 5.8160 | ValROUGE-1 0.2025 | elapsed 5.6 min\n","Epoch   4 | TrainLoss 5.3713 | ValROUGE-1 0.2335 | elapsed 7.4 min\n","Epoch   5 | TrainLoss 4.9465 | ValROUGE-1 0.2794 | elapsed 9.3 min\n","Epoch   6 | TrainLoss 4.5680 | ValROUGE-1 0.3116 | elapsed 11.1 min\n","Epoch   7 | TrainLoss 4.2314 | ValROUGE-1 0.3416 | elapsed 13.0 min\n","Epoch   8 | TrainLoss 3.9272 | ValROUGE-1 0.3693 | elapsed 14.9 min\n","Epoch   9 | TrainLoss 3.6620 | ValROUGE-1 0.4061 | elapsed 16.7 min\n","Epoch  10 | TrainLoss 3.4233 | ValROUGE-1 0.4644 | elapsed 18.6 min\n","Epoch  11 | TrainLoss 3.2140 | ValROUGE-1 0.4633 | elapsed 20.4 min\n","Epoch  12 | TrainLoss 3.0325 | ValROUGE-1 0.5122 | elapsed 22.3 min\n","Epoch  13 | TrainLoss 2.8743 | ValROUGE-1 0.5357 | elapsed 24.1 min\n","Epoch  14 | TrainLoss 2.7545 | ValROUGE-1 0.5632 | elapsed 26.0 min\n","Epoch  15 | TrainLoss 2.6594 | ValROUGE-1 0.5877 | elapsed 27.8 min\n","Epoch  16 | TrainLoss 2.5582 | ValROUGE-1 0.6084 | elapsed 29.7 min\n","Epoch  17 | TrainLoss 2.4866 | ValROUGE-1 0.6213 | elapsed 31.5 min\n","Epoch  18 | TrainLoss 2.4235 | ValROUGE-1 0.6331 | elapsed 33.4 min\n","Epoch  19 | TrainLoss 2.3755 | ValROUGE-1 0.6450 | elapsed 35.2 min\n","Epoch  20 | TrainLoss 2.3361 | ValROUGE-1 0.6359 | elapsed 37.1 min\n","Epoch  21 | TrainLoss 2.2941 | ValROUGE-1 0.6384 | elapsed 38.9 min\n","Epoch  22 | TrainLoss 2.2680 | ValROUGE-1 0.6563 | elapsed 40.8 min\n","Epoch  23 | TrainLoss 2.2437 | ValROUGE-1 0.6596 | elapsed 42.6 min\n","Epoch  24 | TrainLoss 2.2202 | ValROUGE-1 0.6678 | elapsed 44.4 min\n","Epoch  25 | TrainLoss 2.1951 | ValROUGE-1 0.6682 | elapsed 46.3 min\n","Epoch  26 | TrainLoss 2.1830 | ValROUGE-1 0.6818 | elapsed 48.1 min\n","Epoch  27 | TrainLoss 2.1683 | ValROUGE-1 0.6688 | elapsed 50.0 min\n","Epoch  28 | TrainLoss 2.1572 | ValROUGE-1 0.6824 | elapsed 51.8 min\n","Epoch  29 | TrainLoss 2.1452 | ValROUGE-1 0.6778 | elapsed 53.7 min\n","Epoch  30 | TrainLoss 2.1393 | ValROUGE-1 0.6891 | elapsed 55.5 min\n","Epoch  31 | TrainLoss 2.1283 | ValROUGE-1 0.6804 | elapsed 57.3 min\n","Epoch  32 | TrainLoss 2.1288 | ValROUGE-1 0.6863 | elapsed 59.2 min\n","Epoch  33 | TrainLoss 2.1189 | ValROUGE-1 0.6857 | elapsed 61.0 min\n","Epoch  34 | TrainLoss 2.1166 | ValROUGE-1 0.6739 | elapsed 62.9 min\n","Epoch  35 | TrainLoss 2.1208 | ValROUGE-1 0.6761 | elapsed 64.8 min\n","Epoch  36 | TrainLoss 2.1169 | ValROUGE-1 0.6799 | elapsed 66.6 min\n","Epoch  37 | TrainLoss 2.1126 | ValROUGE-1 0.6725 | elapsed 68.5 min\n","Epoch  38 | TrainLoss 2.1181 | ValROUGE-1 0.6668 | elapsed 70.3 min\n","Epoch  39 | TrainLoss 2.1160 | ValROUGE-1 0.6818 | elapsed 72.2 min\n","Epoch  40 | TrainLoss 2.1268 | ValROUGE-1 0.6811 | elapsed 74.0 min\n","Epoch  41 | TrainLoss 2.1262 | ValROUGE-1 0.6771 | elapsed 75.9 min\n","Epoch  42 | TrainLoss 2.1249 | ValROUGE-1 0.6741 | elapsed 77.7 min\n","Epoch  43 | TrainLoss 2.1189 | ValROUGE-1 0.6800 | elapsed 79.6 min\n","Epoch  44 | TrainLoss 2.1206 | ValROUGE-1 0.6866 | elapsed 81.4 min\n","Epoch  45 | TrainLoss 2.1179 | ValROUGE-1 0.6926 | elapsed 83.3 min\n","Epoch  46 | TrainLoss 2.1197 | ValROUGE-1 0.6919 | elapsed 85.1 min\n","Epoch  47 | TrainLoss 2.1197 | ValROUGE-1 0.6781 | elapsed 87.0 min\n","Epoch  48 | TrainLoss 2.1134 | ValROUGE-1 0.6661 | elapsed 88.8 min\n","Epoch  49 | TrainLoss 2.1162 | ValROUGE-1 0.6747 | elapsed 90.7 min\n","Epoch  50 | TrainLoss 2.1100 | ValROUGE-1 0.6604 | elapsed 92.6 min\n","Epoch  51 | TrainLoss 2.1073 | ValROUGE-1 0.6828 | elapsed 94.4 min\n","Epoch  52 | TrainLoss 2.1108 | ValROUGE-1 0.6769 | elapsed 96.2 min\n","Epoch  53 | TrainLoss 2.1073 | ValROUGE-1 0.6742 | elapsed 98.1 min\n","Epoch  54 | TrainLoss 2.1068 | ValROUGE-1 0.6727 | elapsed 99.9 min\n","Epoch  55 | TrainLoss 2.1030 | ValROUGE-1 0.6626 | elapsed 101.8 min\n","Epoch  56 | TrainLoss 2.1015 | ValROUGE-1 0.6304 | elapsed 103.6 min\n","Epoch  57 | TrainLoss 2.1001 | ValROUGE-1 0.6009 | elapsed 105.5 min\n","Epoch  58 | TrainLoss 2.0957 | ValROUGE-1 0.5994 | elapsed 107.3 min\n","Epoch  59 | TrainLoss 2.0985 | ValROUGE-1 0.5993 | elapsed 109.2 min\n","Epoch  60 | TrainLoss 2.0943 | ValROUGE-1 0.5664 | elapsed 111.0 min\n","Epoch  61 | TrainLoss 2.0943 | ValROUGE-1 0.4761 | elapsed 112.9 min\n","Epoch  62 | TrainLoss 2.0887 | ValROUGE-1 0.4988 | elapsed 114.7 min\n","Epoch  63 | TrainLoss 2.0920 | ValROUGE-1 0.4784 | elapsed 116.6 min\n","Epoch  64 | TrainLoss 2.0941 | ValROUGE-1 0.5131 | elapsed 118.5 min\n","Epoch  65 | TrainLoss 2.0834 | ValROUGE-1 0.4567 | elapsed 120.3 min\n","Epoch  66 | TrainLoss 2.0895 | ValROUGE-1 0.5000 | elapsed 122.2 min\n","Epoch  67 | TrainLoss 2.0824 | ValROUGE-1 0.5113 | elapsed 124.0 min\n","Epoch  68 | TrainLoss 2.0795 | ValROUGE-1 0.4874 | elapsed 125.9 min\n","Epoch  69 | TrainLoss 2.0823 | ValROUGE-1 0.4845 | elapsed 127.7 min\n","Epoch  70 | TrainLoss 2.0782 | ValROUGE-1 0.5210 | elapsed 129.6 min\n","Epoch  71 | TrainLoss 2.0844 | ValROUGE-1 0.4817 | elapsed 131.4 min\n","Epoch  72 | TrainLoss 2.0826 | ValROUGE-1 0.3678 | elapsed 133.3 min\n","Epoch  73 | TrainLoss 2.0794 | ValROUGE-1 0.3739 | elapsed 135.1 min\n","Epoch  74 | TrainLoss 2.0783 | ValROUGE-1 0.3299 | elapsed 137.0 min\n","Epoch  75 | TrainLoss 2.0822 | ValROUGE-1 0.3245 | elapsed 138.8 min\n","Epoch  76 | TrainLoss 2.0790 | ValROUGE-1 0.2876 | elapsed 140.7 min\n","Epoch  77 | TrainLoss 2.0785 | ValROUGE-1 0.3030 | elapsed 142.5 min\n","Epoch  78 | TrainLoss 2.0745 | ValROUGE-1 0.3098 | elapsed 144.4 min\n","Epoch  79 | TrainLoss 2.0679 | ValROUGE-1 0.2911 | elapsed 146.2 min\n","Epoch  80 | TrainLoss 2.0683 | ValROUGE-1 0.3220 | elapsed 148.1 min\n","Epoch  81 | TrainLoss 2.0712 | ValROUGE-1 0.2958 | elapsed 149.9 min\n","Epoch  82 | TrainLoss 2.0766 | ValROUGE-1 0.3040 | elapsed 151.7 min\n","Epoch  83 | TrainLoss 2.0804 | ValROUGE-1 0.3306 | elapsed 153.6 min\n","Epoch  84 | TrainLoss 2.0777 | ValROUGE-1 0.3440 | elapsed 155.4 min\n","Epoch  85 | TrainLoss 2.0743 | ValROUGE-1 0.3780 | elapsed 157.3 min\n","Epoch  86 | TrainLoss 2.0783 | ValROUGE-1 0.3757 | elapsed 159.2 min\n","Epoch  87 | TrainLoss 2.0806 | ValROUGE-1 0.4029 | elapsed 161.0 min\n","Epoch  88 | TrainLoss 2.0784 | ValROUGE-1 0.3535 | elapsed 162.8 min\n","Epoch  89 | TrainLoss 2.0778 | ValROUGE-1 0.3836 | elapsed 164.7 min\n","Epoch  90 | TrainLoss 2.0741 | ValROUGE-1 0.2943 | elapsed 166.6 min\n","Epoch  91 | TrainLoss 2.0697 | ValROUGE-1 0.2934 | elapsed 168.4 min\n","Epoch  92 | TrainLoss 2.0708 | ValROUGE-1 0.3502 | elapsed 170.3 min\n","Epoch  93 | TrainLoss 2.0744 | ValROUGE-1 0.3063 | elapsed 172.1 min\n","Epoch  94 | TrainLoss 2.0754 | ValROUGE-1 0.3734 | elapsed 174.0 min\n","Epoch  95 | TrainLoss 2.0782 | ValROUGE-1 0.3395 | elapsed 175.9 min\n","Epoch  96 | TrainLoss 2.0737 | ValROUGE-1 0.3514 | elapsed 177.7 min\n","Epoch  97 | TrainLoss 2.0741 | ValROUGE-1 0.3488 | elapsed 179.6 min\n","Epoch  98 | TrainLoss 2.0762 | ValROUGE-1 0.4041 | elapsed 181.4 min\n","Epoch  99 | TrainLoss 2.0818 | ValROUGE-1 0.3346 | elapsed 183.3 min\n","Epoch 100 | TrainLoss 2.0940 | ValROUGE-1 0.3739 | elapsed 185.1 min\n","[save] model saved to /content/drive/MyDrive/Transformer/model6.pt\n","Training finished.\n"]}]},{"cell_type":"markdown","source":["# Trial 7"],"metadata":{"id":"sI3OmTgMiLJL"}},{"cell_type":"code","source":["# train_transformer_fixed_decode.py (Colab-ready cell)\n","# Requirements: tokenizers, torch, rouge_score, pandas\n","import os\n","import math\n","import time\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","from rouge_score import rouge_scorer\n","\n","# ========== CONFIG ==========\n","DATA_CSV = \"/content/drive/MyDrive/Transformer/mtsamples.csv\"   # change if needed\n","SAVE_DIR = \"/content/drive/MyDrive/Transformer\"\n","TOKENIZER_NAME = \"tokenizer7.json\"\n","MODEL_NAME = \"model7.pt\"\n","\n","VOCAB_SIZE = 30000\n","MAX_SRC_LEN = 512\n","MAX_TGT_LEN = 256\n","BATCH_SIZE = 4\n","NUM_EPOCHS = 20    # set low to test; increase later\n","LR = 1e-4\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# ========== 1. Load data ==========\n","if not os.path.exists(DATA_CSV):\n","    raise FileNotFoundError(f\"Data CSV not found at {DATA_CSV}. Upload to Drive or change DATA_CSV.\")\n","df = pd.read_csv(DATA_CSV)\n","df = df[['transcription', 'description']].dropna().rename(columns={'transcription':'text','description':'summary'})\n","print(f\"[data] {len(df)} rows. sample:\")\n","print(df.head(2).to_dict(orient='records'))\n","\n","# ========== 2. Train tokenizer (WordPiece) ==========\n","special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n","\n","print(\"[tokenizer] training (may take a while)...\")\n","def iter_texts():\n","    for t,s in zip(df['text'].astype(str), df['summary'].astype(str)):\n","        yield t\n","        yield s\n","tokenizer.train_from_iterator(iter_texts(), trainer=trainer)\n","tokenizer_path = os.path.join(SAVE_DIR, TOKENIZER_NAME)\n","tokenizer.save(tokenizer_path)\n","print(f\"[tokenizer] saved to {tokenizer_path}\")\n","\n","CLS_ID = tokenizer.token_to_id(\"[CLS]\")\n","SEP_ID = tokenizer.token_to_id(\"[SEP]\")\n","PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n","UNK_ID = tokenizer.token_to_id(\"[UNK]\")\n","print(f\"[tokenizer ids] CLS:{CLS_ID} SEP:{SEP_ID} PAD:{PAD_ID} UNK:{UNK_ID}  vocab_size={tokenizer.get_vocab_size()}\")\n","assert None not in (CLS_ID, SEP_ID, PAD_ID, UNK_ID), \"Missing special token — retrain with special_tokens set.\"\n","\n","# ========== 3. Encode helper ==========\n","def encode_pair(text: str, summary: str):\n","    src_ids = tokenizer.encode(str(text)).ids[:MAX_SRC_LEN]\n","    tgt_ids = tokenizer.encode(str(summary)).ids\n","    tgt = [CLS_ID] + tgt_ids + [SEP_ID]\n","    src_ids = src_ids[:MAX_SRC_LEN]\n","    tgt = tgt[:MAX_TGT_LEN]\n","    src_ids += [PAD_ID] * (MAX_SRC_LEN - len(src_ids))\n","    tgt += [PAD_ID] * (MAX_TGT_LEN - len(tgt))\n","    return src_ids, tgt\n","\n","print(\"[encode] encoding dataset...\")\n","pairs = [encode_pair(t, s) for t, s in zip(df['text'], df['summary'])]\n","srcs, tgts = zip(*pairs)\n","src_tensor = torch.tensor(srcs, dtype=torch.long)\n","tgt_tensor = torch.tensor(tgts, dtype=torch.long)\n","print(\"[encode] shapes:\", src_tensor.shape, tgt_tensor.shape)\n","\n","# split\n","train_size = int(0.8 * len(src_tensor))\n","train_src, val_src = src_tensor[:train_size], src_tensor[train_size:]\n","train_tgt, val_tgt = tgt_tensor[:train_size], tgt_tensor[train_size:]\n","print(f\"[split] train {len(train_src)} val {len(val_src)}\")\n","\n","# ========== 4. Model (with proper mask support) ==========\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, nhead=8,\n","                 num_encoder_layers=6, num_decoder_layers=6,\n","                 dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n","        self.pos = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n","                                          num_encoder_layers=num_encoder_layers,\n","                                          num_decoder_layers=num_decoder_layers,\n","                                          dim_feedforward=dim_feedforward,\n","                                          dropout=dropout,\n","                                          batch_first=True)\n","        self.out = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, src, tgt, tgt_mask=None,\n","                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n","        # embed + positional\n","        src_emb = self.embed(src) * math.sqrt(self.d_model)\n","        tgt_emb = self.embed(tgt) * math.sqrt(self.d_model)\n","        src_emb = self.pos(src_emb)\n","        tgt_emb = self.pos(tgt_emb)\n","        out = self.transformer(src_emb, tgt_emb,\n","                               tgt_mask=tgt_mask,\n","                               src_key_padding_mask=src_key_padding_mask,\n","                               tgt_key_padding_mask=tgt_key_padding_mask,\n","                               memory_key_padding_mask=memory_key_padding_mask)\n","        logits = self.out(out)\n","        return logits\n","\n","    @staticmethod\n","    def subsequent_mask(sz, device):\n","        return torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n","\n","# instantiate\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(vocab_size=vocab_size).to(DEVICE)\n","print(f\"[model] vocab_size={vocab_size} device={DEVICE}\")\n","\n","# ========== 5. Training utilities ==========\n","optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9,0.98), eps=1e-9)\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n","scaler = GradScaler()\n","\n","class SimpleDataset(torch.utils.data.Dataset):\n","    def __init__(self, srcs, tgts):\n","        self.srcs = srcs\n","        self.tgts = tgts\n","    def __len__(self):\n","        return len(self.srcs)\n","    def __getitem__(self, idx):\n","        return {'src': self.srcs[idx], 'tgt': self.tgts[idx]}\n","\n","train_ds = SimpleDataset(train_src, train_tgt)\n","val_ds = SimpleDataset(val_src, val_tgt)\n","train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE)\n","\n","# ========== 6. Greedy decode function (used in validation) ==========\n","def greedy_decode(model, src_tensor, tokenizer, max_len=MAX_TGT_LEN, device=DEVICE):\n","    # src_tensor: (B, S)\n","    model.eval()\n","    batch = src_tensor.size(0)\n","    results = []\n","    src_key_padding = (src_tensor == PAD_ID)  # (B,S)\n","    with torch.no_grad():\n","        for i in range(batch):\n","            src = src_tensor[i:i+1].to(device)\n","            src_key_pad = src_key_padding[i:i+1].to(device)\n","            generated = [CLS_ID]\n","            for _ in range(max_len - 1):\n","                # build tgt full padded sequence\n","                tgt_full = generated + [PAD_ID] * (max_len - len(generated))\n","                tgt_tensor = torch.tensor([tgt_full], dtype=torch.long, device=device)\n","                tgt_key_pad = (tgt_tensor == PAD_ID)  # (1, max_len)\n","                tgt_mask = TransformerModel.subsequent_mask(max_len, device=device)\n","                logits = model(src, tgt_tensor,\n","                               tgt_mask=tgt_mask,\n","                               src_key_padding_mask=src_key_pad,\n","                               tgt_key_padding_mask=tgt_key_pad,\n","                               memory_key_padding_mask=src_key_pad)\n","                # pick logit at last generated position\n","                pos = len(generated) - 1\n","                next_token = int(logits[0, pos].argmax().item())\n","                generated.append(next_token)\n","                if next_token == SEP_ID:\n","                    break\n","                if len(generated) >= max_len:\n","                    break\n","            # trim at SEP\n","            if SEP_ID in generated:\n","                generated = generated[:generated.index(SEP_ID)+1]\n","            results.append(generated)\n","    # decode each — NOTE: do NOT pass unsupported keyword args\n","    decoded = [tokenizer.decode(r, skip_special_tokens=True).strip() for r in results]\n","    return decoded\n","\n","# ========== 7. Training loop ==========\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n","print(\"[train] starting...\")\n","start = time.time()\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    model.train()\n","    running_loss = 0.0\n","    batches = 0\n","    for batch in train_loader:\n","        optimizer.zero_grad()\n","        src = batch['src'].to(DEVICE)\n","        tgt = batch['tgt'].to(DEVICE)\n","        tgt_in = tgt[:, :-1]\n","        tgt_out = tgt[:, 1:]\n","\n","        src_key_pad = (src == PAD_ID)\n","        tgt_key_pad = (tgt_in == PAD_ID)\n","        tgt_mask = TransformerModel.subsequent_mask(tgt_in.size(1), device=DEVICE)\n","\n","        with autocast():\n","            logits = model(src, tgt_in,\n","                           tgt_mask=tgt_mask,\n","                           src_key_padding_mask=src_key_pad,\n","                           tgt_key_padding_mask=tgt_key_pad,\n","                           memory_key_padding_mask=src_key_pad)\n","            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        running_loss += loss.item()\n","        batches += 1\n","\n","    avg_loss = running_loss / max(1, batches)\n","\n","    # validation using greedy decode\n","    model.eval()\n","    r1s = []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            src = batch['src'].to(DEVICE)\n","            tgt = batch['tgt'].to(DEVICE)\n","            preds = greedy_decode(model, src, tokenizer, max_len=MAX_TGT_LEN, device=DEVICE)  # list of strings\n","            # get references\n","            for p_str, t in zip(preds, tgt):\n","                # build reference string (drop starting CLS and anything after SEP)\n","                ref_ids = t[1:].tolist()\n","                if SEP_ID in ref_ids:\n","                    ref_ids = ref_ids[:ref_ids.index(SEP_ID)]\n","                ref_ids = [x for x in ref_ids if x != PAD_ID]\n","                # decode (no unsupported kwargs)\n","                ref_text = tokenizer.decode(ref_ids, skip_special_tokens=True).strip()\n","                gen_text = p_str.strip()\n","                if ref_text and gen_text:\n","                    r1s.append(scorer.score(ref_text, gen_text)['rouge1'].fmeasure)\n","\n","    val_rouge1 = sum(r1s)/len(r1s) if r1s else 0.0\n","    elapsed = (time.time() - start) / 60.0\n","    print(f\"Epoch {epoch:3d} | TrainLoss {avg_loss:.4f} | ValROUGE-1 {val_rouge1:.4f} | elapsed {elapsed:.1f} min\")\n","\n","# ========== 8. Save ==========\n","torch.save(model.state_dict(), os.path.join(SAVE_DIR, MODEL_NAME))\n","print(f\"[save] model saved to {os.path.join(SAVE_DIR, MODEL_NAME)}\")\n","print(\"Training finished.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":671},"id":"Ps0aldM8iQdj","outputId":"d38f8b63-11ae-485c-d5ca-03f004b8e8b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[data] 4966 rows. sample:\n","[{'text': 'SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.', 'summary': ' A 23-year-old white female presents with complaint of allergies.'}, {'text': 'PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off the floor.  He exercises three times a week at home and does cardio.  He has difficulty walking two blocks or five flights of stairs.  Difficulty with snoring.  He has muscle and joint pains including knee pain, back pain, foot and ankle pain, and swelling.  He has gastroesophageal reflux disease.,PAST SURGICAL HISTORY:, Includes reconstructive surgery on his right hand 13 years ago.  ,SOCIAL HISTORY:, He is currently single.  He has about ten drinks a year.  He had smoked significantly up until several months ago.  He now smokes less than three cigarettes a day.,FAMILY HISTORY:, Heart disease in both grandfathers, grandmother with stroke, and a grandmother with diabetes.  Denies obesity and hypertension in other family members.,CURRENT MEDICATIONS:, None.,ALLERGIES:,  He is allergic to Penicillin.,MISCELLANEOUS/EATING HISTORY:, He has been going to support groups for seven months with Lynn Holmberg in Greenwich and he is from Eastchester, New York and he feels that we are the appropriate program.  He had a poor experience with the Greenwich program.  Eating history, he is not an emotional eater.  Does not like sweets.  He likes big portions and carbohydrates.  He likes chicken and not steak.  He currently weighs 312 pounds.  Ideal body weight would be 170 pounds.  He is 142 pounds overweight.  If ,he lost 60% of his excess body weight that would be 84 pounds and he should weigh about 228.,REVIEW OF SYSTEMS: ,Negative for head, neck, heart, lungs, GI, GU, orthopedic, and skin.  Specifically denies chest pain, heart attack, coronary artery disease, congestive heart failure, arrhythmia, atrial fibrillation, pacemaker, high cholesterol, pulmonary embolism, high blood pressure, CVA, venous insufficiency, thrombophlebitis, asthma, shortness of breath, COPD, emphysema, sleep apnea, diabetes, leg and foot swelling, osteoarthritis, rheumatoid arthritis, hiatal hernia, peptic ulcer disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, hemorrhoids, rectal bleeding, polyps, incontinence of stool, urinary stress incontinence, or cancer.  Denies cellulitis, pseudotumor cerebri, meningitis, or encephalitis.,PHYSICAL EXAMINATION:, He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Afebrile.  Vital Signs are stable.', 'summary': ' Consult for laparoscopic gastric bypass.'}]\n","[tokenizer] training (may take a while)...\n","[tokenizer] saved to /content/drive/MyDrive/Transformer/tokenizer7.json\n","[tokenizer ids] CLS:2 SEP:3 PAD:0 UNK:1  vocab_size=30000\n","[encode] encoding dataset...\n","[encode] shapes: torch.Size([4966, 512]) torch.Size([4966, 256])\n","[split] train 3972 val 994\n","[model] vocab_size=30000 device=cuda\n","[train] starting...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1128662419.py:144: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-1128662419.py:218: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch   1 | TrainLoss 7.2001 | ValROUGE-1 0.0660 | elapsed 69.5 min\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1128662419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tgt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TGT_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# list of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;31m# get references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1128662419.py\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(model, src_tensor, tokenizer, max_len, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mtgt_key_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt_tensor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPAD_ID\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 logits = model(src, tgt_tensor,\n\u001b[0m\u001b[1;32m    179\u001b[0m                                \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                                \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1128662419.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, tgt_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0msrc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtgt_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         out = self.transformer(src_emb, tgt_emb,\n\u001b[0m\u001b[1;32m    125\u001b[0m                                \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    272\u001b[0m             )\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         memory = self.encoder(\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    900\u001b[0m                     \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                 )\n\u001b[0;32m--> 902\u001b[0;31m                 return torch._transformer_encoder_layer_fwd(\n\u001b[0m\u001b[1;32m    903\u001b[0m                     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# Test 8"],"metadata":{"id":"qOfxLwCmJZIJ"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import math\n","import os\n","from rouge_score import rouge_scorer\n","\n","# --- Configuration (MUST MATCH Frontend) ---\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","D_MODEL = 512\n","DIM_FEEDFORWARD = 2048\n","N_LAYERS = 4\n","N_HEADS = 8\n","BATCH_SIZE = 8\n","NUM_EPOCHS = 50\n","WARMUP_STEPS = 2000 # Critical for Transformer stability\n","GRADIENT_CLIP_NORM = 1.0\n","# -------------------------------------------\n","\n","# --- Step 1 & 2: Data Preparation (Keeping data prep logic consistent) ---\n","df = pd.read_csv('/content/drive/MyDrive/Transformer/mtsamples.csv')\n","df = df[['transcription', 'description']].dropna()\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","\n","texts = list(df['text']) + list(df['summary'])\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","tokenizer.train_from_iterator(texts, trainer=trainer)\n","\n","tokenizer_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(tokenizer_path, exist_ok=True)\n","tokenizer.save(os.path.join(tokenizer_path, \"tokenizer8.json\")) # Updated filename\n","\n","def encode_texts(texts, summaries, max_input_length=512, max_target_length=128):\n","    input_encodings = []\n","    target_encodings = []\n","    cls_token = tokenizer.token_to_id(\"[CLS]\")\n","    sep_token = tokenizer.token_to_id(\"[SEP]\")\n","\n","    for text, summary in zip(texts, summaries):\n","        # Input: [CLS] + Text tokens\n","        input_ids = [cls_token] + tokenizer.encode(text).ids\n","        # Target: [CLS] + Summary tokens\n","        target_ids = [cls_token] + tokenizer.encode(summary).ids\n","\n","        # Apply truncation and add [SEP]\n","        if len(input_ids) > max_input_length - 1:\n","            input_ids = input_ids[:max_input_length-1]\n","        input_ids += [sep_token]\n","\n","        if len(target_ids) > max_target_length - 1:\n","            target_ids = target_ids[:max_target_length-1]\n","        target_ids += [sep_token]\n","\n","        # Final safety truncation\n","        input_encodings.append(input_ids[:max_input_length])\n","        target_encodings.append(target_ids[:max_target_length])\n","\n","    return input_encodings, target_encodings\n","\n","input_encodings, target_encodings = encode_texts(df['text'], df['summary'])\n","\n","def pad_sequences(sequences, max_length, pad_token_id):\n","    padded = []\n","    for seq in sequences:\n","        if len(seq) < max_length:\n","            seq = seq + [pad_token_id] * (max_length - len(seq))\n","        padded.append(seq[:max_length])\n","    return padded\n","\n","max_input_length = 512\n","max_target_length = 128\n","pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n","input_encodings = pad_sequences(input_encodings, max_input_length, pad_token_id)\n","target_encodings = pad_sequences(target_encodings, max_target_length, pad_token_id)\n","\n","input_tensors = torch.tensor(input_encodings, dtype=torch.long)\n","target_tensors = torch.tensor(target_encodings, dtype=torch.long)\n","\n","train_size = int(0.8 * len(input_tensors))\n","train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n","train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n","\n","# --- Step 3: Define Transformer model (Keeping architecture) ---\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :].to(x.device)\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=D_MODEL, nhead=N_HEADS, num_encoder_layers=N_LAYERS, num_decoder_layers=N_LAYERS, dim_feedforward=DIM_FEEDFORWARD, dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n","        src = self.embedding(src) * math.sqrt(self.d_model)\n","        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        tgt = self.pos_encoder(tgt)\n","\n","        output = self.transformer(\n","            src,\n","            tgt,\n","            src_mask=src_mask,\n","            tgt_mask=tgt_mask,\n","            src_key_padding_mask=src_key_padding_mask,\n","            tgt_key_padding_mask=tgt_key_padding_mask\n","        )\n","        output = self.fc_out(output)\n","        return output\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","        return mask.to(device)\n","\n","# --- NEW: Transformer Learning Rate Scheduler ---\n","class NoamOpt:\n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self._rate = 0\n","\n","    def step(self):\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","\n","    def rate(self, step=None):\n","        if step is None:\n","            step = self._step\n","        if step == 0:\n","            return 1e-6\n","\n","        # Formula: d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n","        return self.model_size**(-0.5) * min(step**(-0.5), step * self.warmup_steps**(-1.5))\n","\n","    def zero_grad(self):\n","        self.optimizer.zero_grad()\n","\n","\n","# Instantiate model\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(vocab_size=vocab_size).to(device)\n","\n","# --- Step 4: Training setup (Modified for NoamOpt) ---\n","base_optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n","optimizer = NoamOpt(D_MODEL, WARMUP_STEPS, base_optimizer) # Use NoamOpt wrapper\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n","scaler = GradScaler()\n","\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs = inputs\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': self.inputs[idx], 'labels': self.targets[idx]}\n","\n","train_dataset = MedicalDataset(train_inputs, train_targets)\n","val_dataset = MedicalDataset(val_inputs, val_targets)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","\n","# --- Step 5: Training loop (Modified for NoamOpt & Gradient Clipping) ---\n","def train_epoch(model, loader, optimizer, criterion, scaler):\n","    model.train()\n","    total_loss = 0\n","    for batch in loader:\n","        optimizer.zero_grad()\n","        src = batch['input_ids'].to(device)\n","        tgt = batch['labels'].to(device)\n","        tgt_input = tgt[:, :-1]\n","        tgt_output = tgt[:, 1:]\n","\n","        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n","        src_padding_mask = (src == pad_token_id).to(device)\n","        tgt_padding_mask = (tgt_input == pad_token_id).to(device)\n","\n","        with autocast():\n","            output = model(\n","                src,\n","                tgt_input,\n","                tgt_mask=tgt_mask,\n","                src_key_padding_mask=src_padding_mask,\n","                tgt_key_padding_mask=tgt_padding_mask\n","            )\n","            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","\n","        # 1. Backprop and scaling\n","        scaler.scale(loss).backward()\n","\n","        # 2. Unscale, Clip, Step (Crucial sequence for stability)\n","        scaler.unscale_(optimizer.optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRADIENT_CLIP_NORM) # Apply clipping\n","\n","        scaler.step(optimizer.optimizer)\n","        scaler.update()\n","        optimizer.step() # Update learning rate using Noam schedule\n","\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","# Evaluation remains the same (without Noam step)\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    # ... (evaluation logic, same as before)\n","    # The evaluation logic is extensive, refer to the previous provided 'evaluate' function\n","    # Ensure you are also using the padding masks in evaluate()\n","    with torch.no_grad():\n","        for batch in loader:\n","            # ... (load data, create masks)\n","            src = batch['input_ids'].to(device)\n","            tgt = batch['labels'].to(device)\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","\n","            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n","            src_padding_mask = (src == pad_token_id).to(device)\n","            tgt_padding_mask = (tgt_input == pad_token_id).to(device)\n","\n","            with autocast():\n","                output = model(\n","                    src,\n","                    tgt_input,\n","                    tgt_mask=tgt_mask,\n","                    src_key_padding_mask=src_padding_mask,\n","                    tgt_key_padding_mask=tgt_padding_mask\n","                )\n","                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","            total_loss += loss.item()\n","            # ... (rest of ROUGE logic)\n","\n","    return total_loss / len(loader), None, None # Returning only loss for simplicity\n","\n","# --- Step 6: Train the model ---\n","print(f\"Starting training on device: {device}\")\n","for epoch in range(NUM_EPOCHS):\n","    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler)\n","    current_lr = optimizer._rate\n","    val_loss, _, _ = evaluate(model, val_loader, criterion)\n","\n","    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6e}\")\n","\n","\n","# --- Step 7: Save the model ---\n","model_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(model_path, exist_ok=True)\n","torch.save(model.state_dict(), os.path.join(model_path, \"model8.pt\")) # Updated filename\n","print(f\"Model saved to: {model_path}\")"],"metadata":{"id":"YmnUPg9zwD7Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"185ddeb1-8e7b-4749-80b8-e9c5c26d4306"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3993465070.py:185: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-3993465070.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Starting training on device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/tmp/ipython-input-3993465070.py:262: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Train Loss: nan, Val Loss: nan, LR: 2.455706e-04\n","Epoch 2/50, Train Loss: nan, Val Loss: nan, LR: 4.911412e-04\n","Epoch 3/50, Train Loss: nan, Val Loss: nan, LR: 7.367119e-04\n","Epoch 4/50, Train Loss: nan, Val Loss: nan, LR: 9.822825e-04\n","Epoch 5/50, Train Loss: nan, Val Loss: nan, LR: 8.865471e-04\n","Epoch 6/50, Train Loss: nan, Val Loss: nan, LR: 8.093031e-04\n","Epoch 7/50, Train Loss: nan, Val Loss: nan, LR: 7.492691e-04\n","Epoch 8/50, Train Loss: nan, Val Loss: nan, LR: 7.008770e-04\n","Epoch 9/50, Train Loss: nan, Val Loss: nan, LR: 6.607932e-04\n","Epoch 10/50, Train Loss: nan, Val Loss: nan, LR: 6.268835e-04\n","Epoch 11/50, Train Loss: nan, Val Loss: nan, LR: 5.977099e-04\n","Epoch 12/50, Train Loss: nan, Val Loss: nan, LR: 5.722637e-04\n","Epoch 13/50, Train Loss: nan, Val Loss: nan, LR: 5.498132e-04\n","Epoch 14/50, Train Loss: nan, Val Loss: nan, LR: 5.298132e-04\n","Epoch 15/50, Train Loss: nan, Val Loss: nan, LR: 5.118482e-04\n","Epoch 16/50, Train Loss: nan, Val Loss: nan, LR: 4.955949e-04\n","Epoch 17/50, Train Loss: nan, Val Loss: nan, LR: 4.807977e-04\n","Epoch 18/50, Train Loss: nan, Val Loss: nan, LR: 4.672514e-04\n","Epoch 19/50, Train Loss: nan, Val Loss: nan, LR: 4.547891e-04\n","Epoch 20/50, Train Loss: nan, Val Loss: nan, LR: 4.432736e-04\n","Epoch 21/50, Train Loss: nan, Val Loss: nan, LR: 4.325907e-04\n","Epoch 22/50, Train Loss: nan, Val Loss: nan, LR: 4.226448e-04\n","Epoch 23/50, Train Loss: nan, Val Loss: nan, LR: 4.133547e-04\n","Epoch 24/50, Train Loss: nan, Val Loss: nan, LR: 4.046515e-04\n","Epoch 25/50, Train Loss: nan, Val Loss: nan, LR: 3.964759e-04\n","Epoch 26/50, Train Loss: nan, Val Loss: nan, LR: 3.887766e-04\n","Epoch 27/50, Train Loss: nan, Val Loss: nan, LR: 3.815091e-04\n","Epoch 28/50, Train Loss: nan, Val Loss: nan, LR: 3.746345e-04\n","Epoch 29/50, Train Loss: nan, Val Loss: nan, LR: 3.681187e-04\n","Epoch 30/50, Train Loss: nan, Val Loss: nan, LR: 3.619313e-04\n","Epoch 31/50, Train Loss: nan, Val Loss: nan, LR: 3.560459e-04\n","Epoch 32/50, Train Loss: nan, Val Loss: nan, LR: 3.504385e-04\n","Epoch 33/50, Train Loss: nan, Val Loss: nan, LR: 3.450880e-04\n","Epoch 34/50, Train Loss: nan, Val Loss: nan, LR: 3.399753e-04\n","Epoch 35/50, Train Loss: nan, Val Loss: nan, LR: 3.350833e-04\n","Epoch 36/50, Train Loss: nan, Val Loss: nan, LR: 3.303966e-04\n","Epoch 37/50, Train Loss: nan, Val Loss: nan, LR: 3.259012e-04\n","Epoch 38/50, Train Loss: nan, Val Loss: nan, LR: 3.215844e-04\n","Epoch 39/50, Train Loss: nan, Val Loss: nan, LR: 3.174348e-04\n","Epoch 40/50, Train Loss: nan, Val Loss: nan, LR: 3.134417e-04\n","Epoch 41/50, Train Loss: nan, Val Loss: nan, LR: 3.095957e-04\n","Epoch 42/50, Train Loss: nan, Val Loss: nan, LR: 3.058878e-04\n","Epoch 43/50, Train Loss: nan, Val Loss: nan, LR: 3.023101e-04\n","Epoch 44/50, Train Loss: nan, Val Loss: nan, LR: 2.988550e-04\n","Epoch 45/50, Train Loss: nan, Val Loss: nan, LR: 2.955157e-04\n","Epoch 46/50, Train Loss: nan, Val Loss: nan, LR: 2.922859e-04\n","Epoch 47/50, Train Loss: nan, Val Loss: nan, LR: 2.891598e-04\n","Epoch 48/50, Train Loss: nan, Val Loss: nan, LR: 2.861319e-04\n","Epoch 49/50, Train Loss: nan, Val Loss: nan, LR: 2.831971e-04\n","Epoch 50/50, Train Loss: nan, Val Loss: nan, LR: 2.803508e-04\n","Model saved to: /content/drive/MyDrive/Transformer\n"]}]},{"cell_type":"markdown","source":["# Testing frontend in colab for gpu"],"metadata":{"id":"HrrXzjUhkSxY"}},{"cell_type":"code","source":["# Install required libraries\n","!pip install streamlit torch tokenizers pandas pyngrok --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0LyYgxkkLBm","outputId":"dd57cf1e-d2d5-48a9-b73d-706ae5281c31","executionInfo":{"status":"ok","timestamp":1763702712083,"user_tz":-330,"elapsed":7881,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n","!unzip ngrok-v3-stable-linux-amd64.zip\n","!mv ngrok /usr/local/bin/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JuPw4VypoGkA","outputId":"b0189470-f2d6-467c-dda1-b748cddf6045","executionInfo":{"status":"ok","timestamp":1763702742802,"user_tz":-330,"elapsed":819,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-11-21 05:25:41--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 13.248.244.96, 35.71.179.82, 75.2.60.68, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|13.248.244.96|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9378086 (8.9M) [application/octet-stream]\n","Saving to: ‘ngrok-v3-stable-linux-amd64.zip’\n","\n","ngrok-v3-stable-lin 100%[===================>]   8.94M  --.-KB/s    in 0.1s    \n","\n","2025-11-21 05:25:42 (61.5 MB/s) - ‘ngrok-v3-stable-linux-amd64.zip’ saved [9378086/9378086]\n","\n","Archive:  ngrok-v3-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"]}]},{"cell_type":"code","source":["import re\n","from pyngrok import ngrok\n","\n","# --- Configuration ---\n","# NOTE: The file names MUST match the files you uploaded in the previous step.\n","COLAB_TOKENIZER_PATH = \"/content/drive/MyDrive/Transformer/tokenizer2.json\"\n","COLAB_MODEL_PATH = \"/content/drive/MyDrive/Transformer/model2.pt\"\n","STREAMLIT_APP_FILE = \"/content/drive/MyDrive/Transformer/app2.py\"\n","\n","# Replace with your actual ngrok token\n","NGROK_TOKEN = \"35kJKjks8eW0SstgS7ceeKcJDkY_2bfXGWWyKq4AFGRvYAcgq\"\n","ngrok.set_auth_token(NGROK_TOKEN)\n","\n","# 1. Read and update the original Streamlit code\n","with open(STREAMLIT_APP_FILE, \"r\") as f:\n","    code = f.read()\n","\n","# Use regex to replace the old Windows paths with the Colab paths\n","code = re.sub(r'tokenizer_path\\s*=\\s*\".*?\"', f'tokenizer_path = \"{COLAB_TOKENIZER_PATH}\"', code, count=1)\n","code = re.sub(r'model_path\\s*=\\s*\".*?\"', f'model_path = \"{COLAB_MODEL_PATH}\"', code, count=1)\n","\n","# 2. Write the modified code to a new temporary file\n","COLAB_APP_FILE = \"app2_colab.py\"\n","with open(COLAB_APP_FILE, \"w\") as f:\n","    f.write(code)\n","\n","print(f\"✅ Paths in {STREAMLIT_APP_FILE} updated to use Colab paths.\")\n","print(f\"✅ ngrok token set up.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpZSyq7tkea0","outputId":"43888ae0-a83e-459e-ed19-19fc50ac168c","executionInfo":{"status":"ok","timestamp":1763702884874,"user_tz":-330,"elapsed":45,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Paths in /content/drive/MyDrive/Transformer/app2.py updated to use Colab paths.\n","✅ ngrok token set up.\n"]}]},{"cell_type":"code","source":["import threading\n","import time\n","from pyngrok import ngrok\n","\n","# Define the port Streamlit will run on\n","PORT = 8501\n","COLAB_APP_FILE = \"app2_colab.py\" # The file generated in the previous step\n","\n","# Function to run the Streamlit app\n","def run_streamlit():\n","    # Run the modified app using Streamlit\n","    !streamlit run $COLAB_APP_FILE --server.port $PORT --server.enableCORS false --server.enableXsrfProtection false\n","\n","# Start Streamlit in a separate thread\n","print(\"⚙️ Starting Streamlit server...\")\n","threading.Thread(target=run_streamlit, daemon=True).start()\n","\n","# Wait a few seconds for Streamlit to start\n","time.sleep(8)\n","\n","# Start ngrok tunnel\n","public_url = ngrok.connect(PORT)\n","print(f\"\\n\\n🎉 Your Streamlit App is now public at the URL below. Click the link to open it:\\n\")\n","print(f\"🔗 Public URL: {public_url}\\n\\n\")\n","\n","# To keep the script running and the tunnel open, the cell must continue to run.\n","try:\n","    while True:\n","        time.sleep(1)\n","except KeyboardInterrupt:\n","    ngrok.kill()\n","    print(\"Tunnel closed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKoS4tIKkt3K","outputId":"5558ae71-d827-433e-d9f4-51ab47359c61","executionInfo":{"status":"ok","timestamp":1763703172180,"user_tz":-330,"elapsed":125794,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["⚙️ Starting Streamlit server...\n","\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n","\u001b[0m\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.138.173.130:8501\u001b[0m\n","\u001b[0m\n","\n","\n","🎉 Your Streamlit App is now public at the URL below. Click the link to open it:\n","\n","🔗 Public URL: NgrokTunnel: \"https://a6be12c2b333.ngrok-free.app\" -> \"http://localhost:8501\"\n","\n","\n","Tunnel closed.\n"]}]},{"cell_type":"markdown","source":["# Test 9"],"metadata":{"id":"in7WT7zp8agJ"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import math\n","import os\n","from rouge_score import rouge_scorer\n","import re\n","\n","# Define model path at the beginning\n","model_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(model_path, exist_ok=True)\n","\n","# Step 1: Load and prepare the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Transformer/mtsamples.csv')\n","df = df[['transcription', 'description']].dropna()\n","\n","# Clean and filter data\n","def clean_text(text):\n","    # Remove special characters and extra whitespace\n","    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","df['text'] = df['transcription'].apply(clean_text)\n","df['summary'] = df['description'].apply(clean_text)\n","\n","# Filter out very short entries\n","df = df[(df['text'].str.len() > 50) & (df['summary'].str.len() > 10)]\n","\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","\n","# Step 2: Train a WordPiece tokenizer\n","texts = list(df['text']) + list(df['summary'])\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(\n","    vocab_size=30000,\n","    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n","    min_frequency=2  # Only include tokens appearing at least twice\n",")\n","tokenizer.train_from_iterator(texts, trainer=trainer)\n","\n","# Save tokenizer\n","tokenizer.save(os.path.join(model_path, \"tokenizer9.json\"))\n","\n","# Encode dataset\n","def encode_texts(texts, summaries, max_input_length=512, max_target_length=128):\n","    input_encodings = []\n","    target_encodings = []\n","    for text, summary in zip(texts, summaries):\n","        input_ids = tokenizer.encode(text).ids\n","        target_ids = tokenizer.encode(summary).ids\n","        # Add start and end tokens\n","        input_ids = [tokenizer.token_to_id(\"[CLS]\")] + input_ids + [tokenizer.token_to_id(\"[SEP]\")]\n","        target_ids = [tokenizer.token_to_id(\"[CLS]\")] + target_ids + [tokenizer.token_to_id(\"[SEP]\")]\n","\n","        if len(input_ids) > max_input_length:\n","            input_ids = input_ids[:max_input_length]\n","        if len(target_ids) > max_target_length:\n","            target_ids = target_ids[:max_target_length]\n","        input_encodings.append(input_ids)\n","        target_encodings.append(target_ids)\n","    return input_encodings, target_encodings\n","\n","input_encodings, target_encodings = encode_texts(df['text'], df['summary'])\n","\n","# Pad sequences\n","def pad_sequences(sequences, max_length, pad_token_id):\n","    padded = []\n","    for seq in sequences:\n","        if len(seq) < max_length:\n","            seq = seq + [pad_token_id] * (max_length - len(seq))\n","        padded.append(seq[:max_length])\n","    return padded\n","\n","max_input_length = 512\n","max_target_length = 128\n","pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n","input_encodings = pad_sequences(input_encodings, max_input_length, pad_token_id)\n","target_encodings = pad_sequences(target_encodings, max_target_length, pad_token_id)\n","\n","# Convert to tensors\n","input_tensors = torch.tensor(input_encodings, dtype=torch.long)\n","target_tensors = torch.tensor(target_encodings, dtype=torch.long)\n","\n","# Split into train and validation (80-20)\n","train_size = int(0.8 * len(input_tensors))\n","train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n","train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n","\n","# Step 3: Define Enhanced Transformer model\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=512):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, nhead=16, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src, tgt, src_key_padding_mask=None, tgt_mask=None):\n","        src = self.embedding(src) * math.sqrt(self.d_model)\n","        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        tgt = self.pos_encoder(tgt)\n","        output = self.transformer(\n","            src, tgt,\n","            tgt_mask=tgt_mask,\n","            src_key_padding_mask=src_key_padding_mask\n","        )\n","        output = self.fc_out(output)\n","        return output\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","        return mask\n","\n","# Instantiate model\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(\n","    vocab_size=vocab_size,\n","    d_model=512,\n","    nhead=16,\n","    num_encoder_layers=6,\n","    num_decoder_layers=6,\n","    dim_feedforward=2048,\n","    dropout=0.1\n",").cuda()\n","\n","# Step 4: Training setup\n","optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-7)\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n","scaler = GradScaler()\n","\n","# Create dataset and dataloader\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs = inputs\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.inputs[idx],\n","            'labels': self.targets[idx],\n","            'src_padding_mask': (self.inputs[idx] == pad_token_id)\n","        }\n","\n","train_dataset = MedicalDataset(train_inputs, train_targets)\n","val_dataset = MedicalDataset(val_inputs, val_targets)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2)\n","\n","# Step 5: Training loop\n","def train_epoch(model, loader, optimizer, criterion, scaler, scheduler):\n","    model.train()\n","    total_loss = 0\n","    for batch in loader:\n","        optimizer.zero_grad()\n","        src = batch['input_ids'].cuda()\n","        tgt = batch['labels'].cuda()\n","        src_padding_mask = batch['src_padding_mask'].cuda()\n","\n","        tgt_input = tgt[:, :-1]\n","        tgt_output = tgt[:, 1:]\n","        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","        with autocast():\n","            output = model(src, tgt_input, src_key_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n","            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","\n","        scaler.scale(loss).backward()\n","        # Gradient clipping\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        # Scheduler step AFTER optimizer step\n","        scheduler.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in loader:\n","            src = batch['input_ids'].cuda()\n","            tgt = batch['labels'].cuda()\n","            src_padding_mask = batch['src_padding_mask'].cuda()\n","\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","            with autocast():\n","                output = model(src, tgt_input, src_key_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n","                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","            total_loss += loss.item()\n","\n","            # Decode for ROUGE\n","            preds = torch.argmax(output, dim=-1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(tgt_output.cpu().numpy())\n","    return total_loss / len(loader), all_preds, all_labels\n","\n","# Step 6: Train the model\n","best_val_loss = float('inf')\n","patience_counter = 0\n","patience = 5\n","\n","for epoch in range(100):\n","    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, scheduler)\n","    val_loss, val_preds, val_labels = evaluate(model, val_loader, criterion)\n","\n","    # Compute ROUGE scores\n","    decoded_preds = []\n","    decoded_labels = []\n","    for pred, label in zip(val_preds, val_labels):\n","        # Remove padding and special tokens\n","        pred = pred[~(pred == pad_token_id)]\n","        label = label[~(label == pad_token_id)]\n","        decoded_preds.append(tokenizer.decode(pred, skip_special_tokens=True))\n","        decoded_labels.append(tokenizer.decode(label, skip_special_tokens=True))\n","\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    rouge_scores = [scorer.score(label, pred) for pred, label in zip(decoded_preds, decoded_labels)]\n","    rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n","\n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, ROUGE-1: {rouge1:.4f}\")\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        # Save best model\n","        torch.save(model.state_dict(), os.path.join(model_path, \"best_model9.pt\"))\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","# Load best model for final save\n","model.load_state_dict(torch.load(os.path.join(model_path, \"best_model9.pt\")))\n","torch.save(model.state_dict(), os.path.join(model_path, \"model9.pt\"))\n","print(f\"Model saved to: {model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGn86c8q8dM5","executionInfo":{"status":"ok","timestamp":1763706671450,"user_tz":-330,"elapsed":183110,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}},"outputId":"30d802fd-1681-4b83-9c7a-8ab0ba4fd893"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1566428489.py:163: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-1566428489.py:201: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n","/tmp/ipython-input-1566428489.py:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 3.4551, Val Loss: 3.4668, ROUGE-1: 0.0000\n","Epoch 2, Train Loss: 3.4414, Val Loss: 1.2334, ROUGE-1: 0.0156\n","Epoch 3, Train Loss: 1.8301, Val Loss: 0.7729, ROUGE-1: 0.0156\n","Epoch 4, Train Loss: 0.8970, Val Loss: 0.6506, ROUGE-1: 0.0157\n","Epoch 5, Train Loss: 0.7224, Val Loss: 0.6506, ROUGE-1: 0.0157\n","Epoch 6, Train Loss: 0.6919, Val Loss: 0.6061, ROUGE-1: 0.0157\n","Epoch 7, Train Loss: 0.5459, Val Loss: 0.5674, ROUGE-1: 1.0000\n","Epoch 8, Train Loss: 0.8589, Val Loss: 0.5568, ROUGE-1: 0.0000\n","Epoch 9, Train Loss: 0.5967, Val Loss: 0.5409, ROUGE-1: 0.0000\n","Epoch 10, Train Loss: 0.5370, Val Loss: 0.4857, ROUGE-1: 0.0161\n","Epoch 11, Train Loss: 0.5636, Val Loss: 0.4806, ROUGE-1: 0.0157\n","Epoch 12, Train Loss: 0.5192, Val Loss: 0.4144, ROUGE-1: 0.0194\n","Epoch 13, Train Loss: 0.4296, Val Loss: 0.3743, ROUGE-1: 1.0000\n","Epoch 14, Train Loss: 0.4457, Val Loss: 0.3340, ROUGE-1: 0.0270\n","Epoch 15, Train Loss: 0.4225, Val Loss: 0.2839, ROUGE-1: 0.0317\n","Epoch 16, Train Loss: 0.3583, Val Loss: 0.2275, ROUGE-1: 1.0000\n","Epoch 17, Train Loss: 0.3910, Val Loss: 0.1765, ROUGE-1: 1.0000\n","Epoch 18, Train Loss: 0.2448, Val Loss: 0.1260, ROUGE-1: 1.0000\n","Epoch 19, Train Loss: 0.2721, Val Loss: 0.0834, ROUGE-1: 1.0000\n","Epoch 20, Train Loss: 0.1367, Val Loss: 0.0560, ROUGE-1: 1.0000\n","Epoch 21, Train Loss: 0.1389, Val Loss: 0.0366, ROUGE-1: 1.0000\n","Epoch 22, Train Loss: 0.0986, Val Loss: 0.0238, ROUGE-1: 1.0000\n","Epoch 23, Train Loss: 0.0729, Val Loss: 0.0149, ROUGE-1: 1.0000\n","Epoch 24, Train Loss: 0.0479, Val Loss: 0.0099, ROUGE-1: 1.0000\n","Epoch 25, Train Loss: 0.0524, Val Loss: 0.0071, ROUGE-1: 1.0000\n","Epoch 26, Train Loss: 0.0295, Val Loss: 0.0053, ROUGE-1: 1.0000\n","Epoch 27, Train Loss: 0.0198, Val Loss: 0.0042, ROUGE-1: 1.0000\n","Epoch 28, Train Loss: 0.0164, Val Loss: 0.0034, ROUGE-1: 1.0000\n","Epoch 29, Train Loss: 0.0239, Val Loss: 0.0028, ROUGE-1: 1.0000\n","Epoch 30, Train Loss: 0.0112, Val Loss: 0.0024, ROUGE-1: 1.0000\n","Epoch 31, Train Loss: 0.0158, Val Loss: 0.0021, ROUGE-1: 1.0000\n","Epoch 32, Train Loss: 0.0091, Val Loss: 0.0019, ROUGE-1: 1.0000\n","Epoch 33, Train Loss: 0.0078, Val Loss: 0.0017, ROUGE-1: 1.0000\n","Epoch 34, Train Loss: 0.0068, Val Loss: 0.0016, ROUGE-1: 1.0000\n","Epoch 35, Train Loss: 0.0076, Val Loss: 0.0015, ROUGE-1: 1.0000\n","Epoch 36, Train Loss: 0.0101, Val Loss: 0.0014, ROUGE-1: 1.0000\n","Epoch 37, Train Loss: 0.0053, Val Loss: 0.0014, ROUGE-1: 1.0000\n","Epoch 38, Train Loss: 0.0085, Val Loss: 0.0013, ROUGE-1: 1.0000\n","Epoch 39, Train Loss: 0.0062, Val Loss: 0.0013, ROUGE-1: 1.0000\n","Epoch 40, Train Loss: 0.0046, Val Loss: 0.0013, ROUGE-1: 1.0000\n","Epoch 41, Train Loss: 0.0053, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 42, Train Loss: 0.0059, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 43, Train Loss: 0.0049, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 44, Train Loss: 0.0049, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 45, Train Loss: 0.0051, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 46, Train Loss: 0.0040, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 47, Train Loss: 0.0048, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 48, Train Loss: 0.0030, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 49, Train Loss: 0.0053, Val Loss: 0.0012, ROUGE-1: 1.0000\n","Epoch 50, Train Loss: 0.0047, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 51, Train Loss: 0.0067, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 52, Train Loss: 0.0041, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 53, Train Loss: 0.0050, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 54, Train Loss: 0.0046, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 55, Train Loss: 0.0044, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 56, Train Loss: 0.0042, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 57, Train Loss: 0.0045, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 58, Train Loss: 0.0045, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 59, Train Loss: 0.0041, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 60, Train Loss: 0.0062, Val Loss: 0.0011, ROUGE-1: 1.0000\n","Epoch 61, Train Loss: 0.0036, Val Loss: 0.0010, ROUGE-1: 1.0000\n","Epoch 62, Train Loss: 0.0040, Val Loss: 0.0010, ROUGE-1: 1.0000\n","Epoch 63, Train Loss: 0.0039, Val Loss: 0.0010, ROUGE-1: 1.0000\n","Epoch 64, Train Loss: 0.0040, Val Loss: 0.0010, ROUGE-1: 1.0000\n","Epoch 65, Train Loss: 0.0034, Val Loss: 0.0009, ROUGE-1: 1.0000\n","Epoch 66, Train Loss: 0.0033, Val Loss: 0.0009, ROUGE-1: 1.0000\n","Epoch 67, Train Loss: 0.0034, Val Loss: 0.0009, ROUGE-1: 1.0000\n","Epoch 68, Train Loss: 0.0041, Val Loss: 0.0008, ROUGE-1: 1.0000\n","Epoch 69, Train Loss: 0.0045, Val Loss: 0.0008, ROUGE-1: 1.0000\n","Epoch 70, Train Loss: 0.0038, Val Loss: 0.0008, ROUGE-1: 1.0000\n","Epoch 71, Train Loss: 0.0030, Val Loss: 0.0007, ROUGE-1: 1.0000\n","Epoch 72, Train Loss: 0.0028, Val Loss: 0.0007, ROUGE-1: 1.0000\n","Epoch 73, Train Loss: 0.0040, Val Loss: 0.0006, ROUGE-1: 1.0000\n","Epoch 74, Train Loss: 0.0024, Val Loss: 0.0006, ROUGE-1: 1.0000\n","Epoch 75, Train Loss: 0.0022, Val Loss: 0.0006, ROUGE-1: 1.0000\n","Epoch 76, Train Loss: 0.0023, Val Loss: 0.0006, ROUGE-1: 1.0000\n","Epoch 77, Train Loss: 0.0023, Val Loss: 0.0005, ROUGE-1: 1.0000\n","Epoch 78, Train Loss: 0.0024, Val Loss: 0.0005, ROUGE-1: 1.0000\n","Epoch 79, Train Loss: 0.0026, Val Loss: 0.0005, ROUGE-1: 1.0000\n","Epoch 80, Train Loss: 0.0020, Val Loss: 0.0005, ROUGE-1: 1.0000\n","Epoch 81, Train Loss: 0.0015, Val Loss: 0.0005, ROUGE-1: 1.0000\n","Epoch 82, Train Loss: 0.0015, Val Loss: 0.0004, ROUGE-1: 1.0000\n","Epoch 83, Train Loss: 0.0016, Val Loss: 0.0004, ROUGE-1: 1.0000\n","Epoch 84, Train Loss: 0.0024, Val Loss: 0.0004, ROUGE-1: 1.0000\n","Epoch 85, Train Loss: 0.0015, Val Loss: 0.0004, ROUGE-1: 1.0000\n","Epoch 86, Train Loss: 0.0015, Val Loss: 0.0004, ROUGE-1: 1.0000\n","Epoch 87, Train Loss: 0.0015, Val Loss: 0.0004, ROUGE-1: 1.0000\n","Epoch 88, Train Loss: 0.0020, Val Loss: 0.0004, ROUGE-1: 1.0000\n","Epoch 89, Train Loss: 0.0016, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 90, Train Loss: 0.0011, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 91, Train Loss: 0.0011, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 92, Train Loss: 0.0009, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 93, Train Loss: 0.0013, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 94, Train Loss: 0.0011, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 95, Train Loss: 0.0010, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 96, Train Loss: 0.0011, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 97, Train Loss: 0.0010, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 98, Train Loss: 0.0009, Val Loss: 0.0003, ROUGE-1: 1.0000\n","Epoch 99, Train Loss: 0.0010, Val Loss: 0.0002, ROUGE-1: 1.0000\n","Epoch 100, Train Loss: 0.0009, Val Loss: 0.0002, ROUGE-1: 1.0000\n","Model saved to: /content/drive/MyDrive/Transformer\n"]}]},{"cell_type":"code","source":["# Create the frontend file\n","frontend_code = '''\n","import streamlit as st\n","import torch\n","from tokenizers import Tokenizer\n","import os\n","import math\n","\n","# Streamlit page configuration\n","st.set_page_config(page_title=\"Medical Transcript Summarizer\", layout=\"wide\")\n","\n","# Load tokenizer\n","tokenizer_path = \"/content/drive/MyDrive/Transformer/tokenizer9.json\"\n","if not os.path.exists(tokenizer_path):\n","    st.error(\"Tokenizer file not found. Ensure tokenizer.json is in your Google Drive.\")\n","    st.stop()\n","tokenizer = Tokenizer.from_file(tokenizer_path)\n","\n","# Define Enhanced Transformer model\n","class PositionalEncoding(torch.nn.Module):\n","    def __init__(self, d_model, max_len=512):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(torch.nn.Module):\n","    def __init__(self, vocab_size, d_model=512, nhead=16, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","        self.embedding = torch.nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        self.transformer = torch.nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.fc_out = torch.nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","    def forward(self, src, tgt, src_key_padding_mask=None, tgt_mask=None):\n","        src = self.embedding(src) * math.sqrt(self.d_model)\n","        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        tgt = self.pos_encoder(tgt)\n","        output = self.transformer(\n","            src, tgt,\n","            tgt_mask=tgt_mask,\n","            src_key_padding_mask=src_key_padding_mask\n","        )\n","        output = self.fc_out(output)\n","        return output\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","        return mask\n","\n","# Load model\n","model_path = \"/content/drive/MyDrive/Transformer/model9.pt\"\n","if not os.path.exists(model_path):\n","    st.error(\"Model file not found. Ensure model.pt is in your Google Drive.\")\n","    st.stop()\n","vocab_size = tokenizer.get_vocab_size()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransformerModel(\n","    vocab_size=vocab_size,\n","    d_model=512,\n","    nhead=16,\n","    num_encoder_layers=6,\n","    num_decoder_layers=6,\n","    dim_feedforward=2048,\n","    dropout=0.1\n",").to(device)\n","model.load_state_dict(torch.load(model_path, map_location=device))\n","model.eval()\n","\n","# Function to generate summary\n","def generate_summary(text, max_length=128, temperature=0.7):\n","    try:\n","        # Encode input text\n","        input_ids = tokenizer.encode(text).ids\n","        input_ids = input_ids[:510]  # Leave space for [CLS] and [SEP]\n","        input_ids = [tokenizer.token_to_id(\"[CLS]\")] + input_ids + [tokenizer.token_to_id(\"[SEP]\")]\n","        input_ids = input_ids + [tokenizer.token_to_id(\"[PAD]\")] * (512 - len(input_ids))\n","        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n","\n","        # Create source padding mask\n","        src_padding_mask = (input_tensor == tokenizer.token_to_id(\"[PAD]\"))\n","\n","        # Initialize target with start token\n","        tgt_ids = [tokenizer.token_to_id(\"[CLS]\")]\n","\n","        for _ in range(max_length):\n","            tgt_tensor = torch.tensor([tgt_ids], dtype=torch.long).to(device)\n","            tgt_mask = model.generate_square_subsequent_mask(tgt_tensor.size(1)).to(device)\n","\n","            with torch.no_grad():\n","                output = model(input_tensor, tgt_tensor, src_key_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n","\n","            # Apply temperature scaling\n","            next_token_logits = output[0, -1, :] / temperature\n","            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n","            next_token = torch.multinomial(next_token_probs, num_samples=1).item()\n","\n","            # Break if end token is generated\n","            if next_token == tokenizer.token_to_id(\"[SEP]\"):\n","                break\n","\n","            tgt_ids.append(next_token)\n","\n","            # Stop if max length reached\n","            if len(tgt_ids) >= max_length:\n","                break\n","\n","        # Decode the generated sequence\n","        summary = tokenizer.decode(tgt_ids, skip_special_tokens=True)\n","        return summary\n","    except Exception as e:\n","        return f\"Error: {str(e)}\"\n","\n","# Streamlit UI\n","st.title(\"Medical Transcript Summarizer\")\n","st.markdown(\"Enter a medical transcript below to generate a concise summary using a Transformer model.\")\n","\n","# Initialize session state\n","if \"generated_summary\" not in st.session_state:\n","    st.session_state.generated_summary = \"\"\n","if \"download_data\" not in st.session_state:\n","    st.session_state.download_data = \"\"\n","\n","# Input section\n","with st.form(key=\"transcript_form\"):\n","    transcript = st.text_area(\n","        \"Medical Transcript\",\n","        placeholder=\"Paste or type your medical transcript here...\",\n","        height=200,\n","        key=\"transcript\"\n","    )\n","    word_count = len(transcript.split()) if transcript.strip() else 0\n","    st.caption(f\"Word count: {word_count}\")\n","    submit_button = st.form_submit_button(\"Generate Summary\")\n","\n","# Output section - Fixed to remove key conflict\n","st.subheader(\"Generated Summary\")\n","# Removed the key parameter to avoid session state conflicts\n","summary_output = st.text_area(\n","    \"Summary\",\n","    value=st.session_state.generated_summary,\n","    height=100,\n","    disabled=True\n",")\n","\n","# Status and actions\n","status = st.empty()\n","col1, col2, col3 = st.columns([1, 1, 1])\n","with col1:\n","    clear_button = st.button(\"Clear\", key=\"clear\")\n","with col2:\n","    copy_button = st.button(\"Copy Summary\", key=\"copy\")\n","with col3:\n","    download_button = st.download_button(\n","        label=\"Download Summary\",\n","        data=st.session_state.download_data,\n","        file_name=\"summary.txt\",\n","        mime=\"text/plain\",\n","        key=\"download\",\n","        disabled=not st.session_state.download_data\n","    )\n","\n","# Handle form submission\n","if submit_button:\n","    if transcript.strip():\n","        status.info(\"Generating summary...\")\n","        summary = generate_summary(transcript)\n","        st.session_state.generated_summary = summary\n","        st.session_state.download_data = summary\n","        status.success(\"Summary generated successfully!\")\n","    else:\n","        status.error(\"Please enter a transcript.\")\n","        st.session_state.generated_summary = \"\"\n","        st.session_state.download_data = \"\"\n","\n","# Handle clear button\n","if clear_button:\n","    st.session_state.transcript = \"\"\n","    st.session_state.generated_summary = \"\"\n","    st.session_state.download_data = \"\"\n","    status.empty()\n","    st.rerun()\n","\n","# Handle copy button\n","if copy_button and st.session_state.generated_summary:\n","    st.write(\"<script>navigator.clipboard.writeText(`{}`)</script>\".format(st.session_state.generated_summary), unsafe_allow_html=True)\n","    status.success(\"Summary copied to clipboard!\")\n","elif copy_button:\n","    status.error(\"No summary to copy.\")\n","\n","# CSS for styling\n","st.markdown(\"\"\"\n","<style>\n","    .stTextArea textarea {\n","        width: 100%;\n","        max-width: 700px;\n","    }\n","    .stButton>button {\n","        width: 150px;\n","    }\n","    .stForm {\n","        background-color: #f5f5f5;\n","        padding: 20px;\n","        border-radius: 10px;\n","        box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n","    }\n","</style>\n","\"\"\", unsafe_allow_html=True)\n","'''\n","\n","with open(\"app.py\", \"w\") as f:\n","    f.write(frontend_code)\n","\n","# Install pyngrok if not already installed\n","!pip install pyngrok\n","\n","# Mount Google Drive to access model files\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set your ngrok authtoken here\n","from pyngrok import ngrok\n","\n","# Replace 'YOUR_AUTHTOKEN_HERE' with your actual ngrok authtoken\n","ngrok.set_auth_token(\"35kJKjks8eW0SstgS7ceeKcJDkY_2bfXGWWyKq4AFGRvYAcgq\")\n","\n","# Run the app\n","import subprocess\n","import time\n","import threading\n","\n","def run_streamlit():\n","    subprocess.run(['streamlit', 'run', 'app.py', '--server.port', '8501'])\n","\n","# Start Streamlit in a separate thread\n","thread = threading.Thread(target=run_streamlit)\n","thread.start()\n","\n","# Wait a moment for the server to start\n","time.sleep(3)\n","\n","# Create ngrok tunnel\n","public_url = ngrok.connect(8501)\n","print(f\"Streamlit app is running at: {public_url}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NWzKk0V-28g","executionInfo":{"status":"ok","timestamp":1763706692831,"user_tz":-330,"elapsed":12315,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}},"outputId":"7d2238d9-eca3-4ac2-9da2-7cfc4195f2b5"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Streamlit app is running at: NgrokTunnel: \"https://99684eb3c7f6.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"markdown","source":["# Test 10"],"metadata":{"id":"1nnapvuAJ-j6"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import math\n","import os\n","from rouge_score import rouge_scorer\n","\n","# Step 1: Load and prepare the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Transformer/mtsamples.csv')\n","df = df[['transcription', 'description']].dropna()\n","df = df.rename(columns={'transcription': 'text', 'description': 'summary'})\n","\n","# Step 2: Train a WordPiece tokenizer on the dataset\n","texts = list(df['text']) + list(df['summary'])\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace()\n","trainer = WordPieceTrainer(vocab_size=30000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","tokenizer.train_from_iterator(texts, trainer=trainer)\n","\n","# Save tokenizer\n","tokenizer_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(tokenizer_path, exist_ok=True)\n","tokenizer.save(os.path.join(tokenizer_path, \"tokenizer10.json\"))\n","\n","# Encode dataset\n","def encode_texts(texts, summaries, max_input_length=512, max_target_length=128):\n","    input_encodings = []\n","    target_encodings = []\n","    for text, summary in zip(texts, summaries):\n","        input_ids = tokenizer.encode(text).ids\n","        target_ids = tokenizer.encode(summary).ids\n","        if len(input_ids) > max_input_length:\n","            input_ids = input_ids[:max_input_length]\n","        if len(target_ids) > max_target_length:\n","            target_ids = target_ids[:max_target_length]\n","        input_encodings.append(input_ids)\n","        target_encodings.append(target_ids)\n","    return input_encodings, target_encodings\n","\n","input_encodings, target_encodings = encode_texts(df['text'], df['summary'])\n","\n","# Pad sequences\n","def pad_sequences(sequences, max_length, pad_token_id):\n","    padded = []\n","    for seq in sequences:\n","        if len(seq) < max_length:\n","            seq = seq + [pad_token_id] * (max_length - len(seq))\n","        padded.append(seq[:max_length])\n","    return padded\n","\n","max_input_length = 512\n","max_target_length = 128\n","pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n","input_encodings = pad_sequences(input_encodings, max_input_length, pad_token_id)\n","target_encodings = pad_sequences(target_encodings, max_target_length, pad_token_id)\n","\n","# Convert to tensors\n","input_tensors = torch.tensor(input_encodings, dtype=torch.long)\n","target_tensors = torch.tensor(target_encodings, dtype=torch.long)\n","\n","# Split into train and validation (80-20)\n","train_size = int(0.8 * len(input_tensors))\n","train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n","train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n","\n","# Step 3: Define Transformer model from scratch\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1), :]\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024, dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True\n","        )\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        self.d_model = d_model\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","        src = self.embedding(src) * math.sqrt(self.d_model)\n","        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        tgt = self.pos_encoder(tgt)\n","        output = self.transformer(src, tgt, src_mask, tgt_mask)\n","        output = self.fc_out(output)\n","        return output\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n","        return mask\n","\n","# Instantiate model\n","vocab_size = tokenizer.get_vocab_size()\n","model = TransformerModel(vocab_size=vocab_size).cuda()\n","\n","# Step 4: Training setup\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n","scaler = GradScaler()\n","\n","# Create dataset and dataloader\n","class MedicalDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs = inputs\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': self.inputs[idx], 'labels': self.targets[idx]}\n","\n","train_dataset = MedicalDataset(train_inputs, train_targets)\n","val_dataset = MedicalDataset(val_inputs, val_targets)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)\n","\n","# Step 5: Training loop\n","def train_epoch(model, loader, optimizer, criterion, scaler):\n","    model.train()\n","    total_loss = 0\n","    for batch in loader:\n","        optimizer.zero_grad()\n","        src = batch['input_ids'].cuda()\n","        tgt = batch['labels'].cuda()\n","        tgt_input = tgt[:, :-1]\n","        tgt_output = tgt[:, 1:]\n","        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","        with autocast():\n","            output = model(src, tgt_input, tgt_mask=tgt_mask)\n","            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in loader:\n","            src = batch['input_ids'].cuda()\n","            tgt = batch['labels'].cuda()\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).cuda()\n","\n","            with autocast():\n","                output = model(src, tgt_input, tgt_mask=tgt_mask)\n","                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n","            total_loss += loss.item()\n","\n","            # Decode for ROUGE\n","            preds = torch.argmax(output, dim=-1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(tgt_output.cpu().numpy())\n","    return total_loss / len(loader), all_preds, all_labels\n","\n","# Step 6: Train the model\n","num_epochs = 100  # More epochs due to training from scratch\n","for epoch in range(num_epochs):\n","    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler)\n","    val_loss, val_preds, val_labels = evaluate(model, val_loader, criterion)\n","\n","    # Compute ROUGE scores\n","    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in val_preds]\n","    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in val_labels]\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    rouge_scores = [scorer.score(label, pred) for pred, label in zip(decoded_preds, decoded_labels)]\n","    rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n","\n","    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, ROUGE-1: {rouge1:.4f}\")\n","\n","# Step 7: Save the model\n","model_path = \"/content/drive/MyDrive/Transformer\"\n","os.makedirs(model_path, exist_ok=True)\n","torch.save(model.state_dict(), os.path.join(model_path, \"model10.pt\"))\n","print(f\"Model saved to: {model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DY-UUAIAJ-Os","executionInfo":{"status":"ok","timestamp":1763711386171,"user_tz":-330,"elapsed":3798092,"user":{"displayName":"Shristi Mishra","userId":"16222547948099418979"}},"outputId":"2786eedc-ec32-42d1-9696-5345d93b2b23"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2957019456.py:124: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipython-input-2957019456.py:156: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n","/tmp/ipython-input-2957019456.py:178: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 6.7383, Val Loss: 6.0683, ROUGE-1: 0.1567\n","Epoch 2, Train Loss: 5.4853, Val Loss: 5.3811, ROUGE-1: 0.1923\n","Epoch 3, Train Loss: 4.7729, Val Loss: 4.9049, ROUGE-1: 0.1910\n","Epoch 4, Train Loss: 4.2098, Val Loss: 4.5147, ROUGE-1: 0.1838\n","Epoch 5, Train Loss: 3.6870, Val Loss: 4.1572, ROUGE-1: 0.2083\n","Epoch 6, Train Loss: 3.2272, Val Loss: 3.8064, ROUGE-1: 0.1485\n","Epoch 7, Train Loss: 2.7963, Val Loss: 3.4718, ROUGE-1: 0.2326\n","Epoch 8, Train Loss: 2.4013, Val Loss: 3.1770, ROUGE-1: 0.1899\n","Epoch 9, Train Loss: 2.0462, Val Loss: 2.8929, ROUGE-1: 0.2047\n","Epoch 10, Train Loss: 1.7241, Val Loss: 2.6538, ROUGE-1: 0.2512\n","Epoch 11, Train Loss: 1.4509, Val Loss: 2.3964, ROUGE-1: 0.2683\n","Epoch 12, Train Loss: 1.2181, Val Loss: 2.2055, ROUGE-1: 0.2766\n","Epoch 13, Train Loss: 1.0291, Val Loss: 2.0071, ROUGE-1: 0.2592\n","Epoch 14, Train Loss: 0.8620, Val Loss: 1.8126, ROUGE-1: 0.2896\n","Epoch 15, Train Loss: 0.7505, Val Loss: 1.7140, ROUGE-1: 0.2686\n","Epoch 16, Train Loss: 0.6345, Val Loss: 1.5619, ROUGE-1: 0.3243\n","Epoch 17, Train Loss: 0.5340, Val Loss: 1.4698, ROUGE-1: 0.3015\n","Epoch 18, Train Loss: 0.4767, Val Loss: 1.3961, ROUGE-1: 0.3899\n","Epoch 19, Train Loss: 0.4176, Val Loss: 1.3580, ROUGE-1: 0.4072\n","Epoch 20, Train Loss: 0.3718, Val Loss: 1.3276, ROUGE-1: 0.4850\n","Epoch 21, Train Loss: 0.3372, Val Loss: 1.3005, ROUGE-1: 0.2684\n","Epoch 22, Train Loss: 0.3082, Val Loss: 1.2667, ROUGE-1: 0.3437\n","Epoch 23, Train Loss: 0.2748, Val Loss: 1.2678, ROUGE-1: 0.3661\n","Epoch 24, Train Loss: 0.2575, Val Loss: 1.2639, ROUGE-1: 0.3618\n","Epoch 25, Train Loss: 0.2408, Val Loss: 1.2910, ROUGE-1: 0.3361\n","Epoch 26, Train Loss: 0.2279, Val Loss: 1.2986, ROUGE-1: 0.2772\n","Epoch 27, Train Loss: 0.2133, Val Loss: 1.2816, ROUGE-1: 0.3320\n","Epoch 28, Train Loss: 0.2026, Val Loss: 1.2749, ROUGE-1: 0.3144\n","Epoch 29, Train Loss: 0.1914, Val Loss: 1.2854, ROUGE-1: 0.3416\n","Epoch 30, Train Loss: 0.1879, Val Loss: 1.2731, ROUGE-1: 0.3354\n","Epoch 31, Train Loss: 0.1708, Val Loss: 1.2687, ROUGE-1: 0.3449\n","Epoch 32, Train Loss: 0.1708, Val Loss: 1.2826, ROUGE-1: 0.3416\n","Epoch 33, Train Loss: 0.1647, Val Loss: 1.2997, ROUGE-1: 0.3634\n","Epoch 34, Train Loss: 0.1648, Val Loss: 1.2902, ROUGE-1: 0.3367\n","Epoch 35, Train Loss: 0.1507, Val Loss: 1.2970, ROUGE-1: 0.2654\n","Epoch 36, Train Loss: 0.1428, Val Loss: 1.3204, ROUGE-1: 0.3254\n","Epoch 37, Train Loss: 0.1420, Val Loss: 1.3256, ROUGE-1: 0.3524\n","Epoch 38, Train Loss: 0.1342, Val Loss: 1.3132, ROUGE-1: 0.3410\n","Epoch 39, Train Loss: 0.1275, Val Loss: 1.3232, ROUGE-1: 0.3217\n","Epoch 40, Train Loss: 0.1229, Val Loss: 1.3194, ROUGE-1: 0.2880\n","Epoch 41, Train Loss: 0.1200, Val Loss: 1.3232, ROUGE-1: 0.3400\n","Epoch 42, Train Loss: 0.1211, Val Loss: 1.3153, ROUGE-1: 0.3088\n","Epoch 43, Train Loss: 0.1125, Val Loss: 1.3178, ROUGE-1: 0.2762\n","Epoch 44, Train Loss: 0.1126, Val Loss: 1.3451, ROUGE-1: 0.3077\n","Epoch 45, Train Loss: 0.1154, Val Loss: 1.3477, ROUGE-1: 0.2915\n","Epoch 46, Train Loss: 0.1062, Val Loss: 1.3495, ROUGE-1: 0.3518\n","Epoch 47, Train Loss: 0.1011, Val Loss: 1.3334, ROUGE-1: 0.3564\n","Epoch 48, Train Loss: 0.1011, Val Loss: 1.3351, ROUGE-1: 0.3741\n","Epoch 49, Train Loss: 0.0950, Val Loss: 1.3657, ROUGE-1: 0.3320\n","Epoch 50, Train Loss: 0.1012, Val Loss: 1.3580, ROUGE-1: 0.2735\n","Epoch 51, Train Loss: 0.0932, Val Loss: 1.3619, ROUGE-1: 0.2612\n","Epoch 52, Train Loss: 0.0913, Val Loss: 1.3746, ROUGE-1: 0.2966\n","Epoch 53, Train Loss: 0.0879, Val Loss: 1.3791, ROUGE-1: 0.2840\n","Epoch 54, Train Loss: 0.0906, Val Loss: 1.3930, ROUGE-1: 0.3090\n","Epoch 55, Train Loss: 0.0845, Val Loss: 1.3948, ROUGE-1: 0.2742\n","Epoch 56, Train Loss: 0.0881, Val Loss: 1.3930, ROUGE-1: 0.3176\n","Epoch 57, Train Loss: 0.0792, Val Loss: 1.3751, ROUGE-1: 0.2611\n","Epoch 58, Train Loss: 0.0736, Val Loss: 1.3947, ROUGE-1: 0.2366\n","Epoch 59, Train Loss: 0.0773, Val Loss: 1.3954, ROUGE-1: 0.2805\n","Epoch 60, Train Loss: 0.0772, Val Loss: 1.3807, ROUGE-1: 0.2917\n","Epoch 61, Train Loss: 0.0766, Val Loss: 1.3963, ROUGE-1: 0.2695\n","Epoch 62, Train Loss: 0.0719, Val Loss: 1.3942, ROUGE-1: 0.2732\n","Epoch 63, Train Loss: 0.0738, Val Loss: 1.3923, ROUGE-1: 0.3196\n","Epoch 64, Train Loss: 0.0665, Val Loss: 1.4103, ROUGE-1: 0.3033\n","Epoch 65, Train Loss: 0.0667, Val Loss: 1.4208, ROUGE-1: 0.2633\n","Epoch 66, Train Loss: 0.0655, Val Loss: 1.3925, ROUGE-1: 0.2869\n","Epoch 67, Train Loss: 0.0639, Val Loss: 1.3901, ROUGE-1: 0.3310\n","Epoch 68, Train Loss: 0.0664, Val Loss: 1.4061, ROUGE-1: 0.2956\n","Epoch 69, Train Loss: 0.0663, Val Loss: 1.4275, ROUGE-1: 0.2691\n","Epoch 70, Train Loss: 0.0632, Val Loss: 1.4176, ROUGE-1: 0.2674\n","Epoch 71, Train Loss: 0.0628, Val Loss: 1.4078, ROUGE-1: 0.3108\n","Epoch 72, Train Loss: 0.0571, Val Loss: 1.4184, ROUGE-1: 0.3575\n","Epoch 73, Train Loss: 0.0619, Val Loss: 1.4398, ROUGE-1: 0.3037\n","Epoch 74, Train Loss: 0.0592, Val Loss: 1.4350, ROUGE-1: 0.3658\n","Epoch 75, Train Loss: 0.0594, Val Loss: 1.4240, ROUGE-1: 0.3346\n","Epoch 76, Train Loss: 0.0580, Val Loss: 1.4282, ROUGE-1: 0.3277\n","Epoch 77, Train Loss: 0.0596, Val Loss: 1.4149, ROUGE-1: 0.3046\n","Epoch 78, Train Loss: 0.0539, Val Loss: 1.4073, ROUGE-1: 0.2922\n","Epoch 79, Train Loss: 0.0510, Val Loss: 1.4360, ROUGE-1: 0.3240\n","Epoch 80, Train Loss: 0.0534, Val Loss: 1.4457, ROUGE-1: 0.3000\n","Epoch 81, Train Loss: 0.0504, Val Loss: 1.4371, ROUGE-1: 0.3368\n","Epoch 82, Train Loss: 0.0552, Val Loss: 1.4521, ROUGE-1: 0.4082\n","Epoch 83, Train Loss: 0.0543, Val Loss: 1.4398, ROUGE-1: 0.3357\n","Epoch 84, Train Loss: 0.0492, Val Loss: 1.4335, ROUGE-1: 0.3246\n","Epoch 85, Train Loss: 0.0475, Val Loss: 1.4560, ROUGE-1: 0.4017\n","Epoch 86, Train Loss: 0.0511, Val Loss: 1.4543, ROUGE-1: 0.3308\n","Epoch 87, Train Loss: 0.0525, Val Loss: 1.4467, ROUGE-1: 0.2765\n","Epoch 88, Train Loss: 0.0469, Val Loss: 1.4464, ROUGE-1: 0.3742\n","Epoch 89, Train Loss: 0.0457, Val Loss: 1.4369, ROUGE-1: 0.3920\n","Epoch 90, Train Loss: 0.0442, Val Loss: 1.4431, ROUGE-1: 0.2820\n","Epoch 91, Train Loss: 0.0448, Val Loss: 1.4461, ROUGE-1: 0.2941\n","Epoch 92, Train Loss: 0.0471, Val Loss: 1.4532, ROUGE-1: 0.2614\n","Epoch 93, Train Loss: 0.0425, Val Loss: 1.4494, ROUGE-1: 0.2485\n","Epoch 94, Train Loss: 0.0432, Val Loss: 1.4638, ROUGE-1: 0.2727\n","Epoch 95, Train Loss: 0.0412, Val Loss: 1.4513, ROUGE-1: 0.2788\n","Epoch 96, Train Loss: 0.0432, Val Loss: 1.4716, ROUGE-1: 0.2953\n","Epoch 97, Train Loss: 0.0432, Val Loss: 1.4814, ROUGE-1: 0.2695\n","Epoch 98, Train Loss: 0.0451, Val Loss: 1.4914, ROUGE-1: 0.2824\n","Epoch 99, Train Loss: 0.0466, Val Loss: 1.4872, ROUGE-1: 0.2720\n","Epoch 100, Train Loss: 0.0406, Val Loss: 1.4740, ROUGE-1: 0.3085\n","Model saved to: /content/drive/MyDrive/Transformer\n"]}]}]}